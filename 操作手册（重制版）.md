# 项目一 Linux的安装与配置
## 工作任务1.1 磁盘格式化 Linux安装环境准备
### 任务活动1.1.1 磁盘格式化

**查看文件类型，如果不是NTFS格式，需格式化**
	![](images/操作手册（重制版）.png)
	![](images/操作手册（重制版）-2.png)

### 任务活动1.1.2 检查CPU虚拟化技术

**查看是否开启虚拟化**
	![](images/操作手册（重制版）-3.png)
	![](images/操作手册（重制版）-4.png)

### 任务活动1.1.3 开启CPU虚拟化技术

**略**

## 工作任务1.2 VMware的安装与配置
### 任务活动1.2.1 安装VMware Workstation16.0

**略**

### 任务活动1.2.2 虚拟网络的配置

**虚拟网络配置**
	![](images/操作手册（重制版）-28.png)
	![](images/操作手册（重制版）-39.png)
	![](images/操作手册（重制版）-50.png)


- **问题一：没有显示“VMnet8”虚拟网卡信息**
	![](images/操作手册（重制版）-61.png)
	![](images/操作手册（重制版）-72.png)

- **问题二：“VMnet8” 虚拟网卡中没有 “默认网关” 信息**
	![](images/操作手册（重制版）-83.png)
	![](images/操作手册（重制版）-86.png)
	![](images/操作手册（重制版）-87.png)
### 任务活动1.2.3 系统服务及虚拟适配器的启停

**略**

## 工作任务1.3 CentOS 7的安装与克隆
### 任务活动1.3.1 CentOS 7的安装

**安装虚拟机**
	![](images/操作手册（重制版）-5.png)
	![](images/操作手册（重制版）-6.png)
	![](images/操作手册（重制版）-7.png)
	![](images/操作手册（重制版）-8.png)
	![](images/操作手册（重制版）-9.png)
	![](images/操作手册（重制版）-10.png)
	![](images/操作手册（重制版）-11.png)
	  ![](images/操作手册（重制版）-12.png)
	![](images/操作手册（重制版）-13.png)
	![](images/操作手册（重制版）-14.png)
	![](images/操作手册（重制版）-15.png)
	![](images/操作手册（重制版）-16.png)
	![](images/操作手册（重制版）-17.png)
	![](images/操作手册（重制版）-19.png)
	![](images/操作手册（重制版）-20.png)


### 任务活动1.3.2 CentOS 7的克隆

**centos 7 的克隆**
	![](images/操作手册（重制版）-25.png)
	![](images/操作手册（重制版）-26.png)
	![](images/操作手册（重制版）-27.png)

# 项目2 Hadoop HDFS高可用集群配置与维护
## 工作任务2.1 Hadoop集群搭建前环境配置
### 任务活动2.1.1 网卡、主机名与IP映射配置


1. **配置网卡**

	**回到CentOS系统窗口，以 node1 虚拟机为例，用 “root” 账号登陆 node1 虚拟机后，输入以下命令**
```shell
vi /etc/sysconfig/network-scripts/ifcfg-ens33
```

​		**在打开的网卡配置文件中，配置静态IP， 其中的 “NETMASK” 和 “GATEWAY” 需要根据在前面步骤中，计算机上的 “VMnet8” 虚拟网卡的子网掩码和网关地址进行填写**
**IPADDR：根据计算机上的 "VMnet8" 虚拟网卡的网关地址所在前三位网段地址 + 主机地址进行填写**

​		**配置文件**

```shell
TYPE=Ethernet
PROXY_METHOD=none
BROWSER_ONLY=no
BOOTPROTO=static    # 从动态获取改为静态获取，将 dhcp 改为 static
DEFROUTE=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
IPV6_ADDR_GEN_MODE=stable-privacy
NAME=ens33
UUID=46c0dea5-7f33-4450-a345-62371f1e0923
DEVICE=ens33
ONBOOT=yes
IPADDR=192.168.40.100    # 根据计算机上的 "VMnet8" 虚拟网卡的网关地址所在前三位网段地址 + 主机地址进行填写
NETMASK=255.255.255.0    # 子网掩码，根据 “VMnet8” 虚拟网卡的子网掩码进行填写
GATEWAY=192.168.40.2    # 网关地址，根据 “VMnet8” 虚拟网卡的网关地址进行填写
DNS1=114.114.114.114    # 域名服务器， 直接按这个写就行
```

重启网卡
```shell
systemctl restart network
```

测试网卡连通性
```shell
ping www.baidu.com
```

2. **配置`SecureCRT`**

	**略**

3. **在所有节点上配置主机名**

	**设置主机名，设置后重启虚拟机**
```shell
# 主机名格式：姓名拼音 + 学号后两位 + 节点名
hostnamectl set-hostname zhanghoulin31-node1    # 在 node1 中执行
hostnamectl set-hostname zhanghoulin31-node2    # 在 node2 中执行
hostnamectl set-hostname zhanghoulin31-node3    # 在 node3 中执行
```

	查看是否设置成功
```shell
hostname
```
结果
	![](images/操作手册（重制版）-88.png)
	![](images/操作手册（重制版）-29.png)
	![](images/操作手册（重制版）-30.png)

4. **在所有节点上修改主机映射**

	**在所有节点上，执行以下代码**

```shell
vi /etc/hosts
```

​	**根据自己的三个节点的IP地址 + 主机名写入文件中， 以下IP内容仅作格式参考，不能作为实际地址写入**
```shell
192.168.40.130 zhanghoulin31-node1
192.168.40.131 zhanghoulin31-node2
192.168.40.132 zhanghoulin31-node3
```

​	**重启网卡**
```shell
systemctl restart network
```

​	**重启系统**
```shell
reboot
```
![](images/操作手册（重制版）-31.png)

### 任务活动2.1.2 Linux常用命令的安装

1. **安装 `ifconfig` 工具**

   **在所有节点上，执行以下命令进行工具的安装**

```shell
yum install -y net-tools.x86_64
```

**成功安装后的反馈信息如下：**
	![](images/操作手册（重制版）-32.png)

2.  **安装 `wget` 工具**

	**在所有节点上，执行以下命令进行工具的安装**

```shell
yum install -y wget.x86_64
```

​	**成功安装后的反馈信息如下**
​	![](images/操作手册（重制版）-33.png)

### 任务活动2.1.3 Linux客户端工具的配置

1. **关闭防火墙**
**查看防火墙的状态**
```shell
systemctl status firewalld
```
​	**关闭防火墙**
```shell
systemctl stop firewalld
```
​	**开机不自启防火墙**
```shell
systemctl disable firewalld
```

2. **关闭`SELinux`**
**查看当前 `SELinux` 状态**
```shell
/usr/sbin/sestatus -v
```
![](images/操作手册（重制版）-51.png)
![](images/操作手册（重制版）-52.png)

​	**如果反馈结果是 “disabled”， 则表示已关闭**
```shell
SELinux status:                 enabled
```
​	**否则执行以下命令，将当前的 `SELinux` 状态改为关闭状态，但系统重启后又会恢复为启动状态**
```shell
setenforce 0
```
![](images/操作手册（重制版）-54.png)
​	**因此，接下来，将 `SELinux` 设置成开机禁用模式，执行以下命令，打开配置文件。**

```shell
vi /etc/sysconfig/selinux
```
​	**在文件中，将 `SELinux`=`enforcing` 改为以下代码**

```shell
SELINUX=disabled
```
![](images/操作手册（重制版）-53.png)

3. **检测THP服务是否被关闭**
**查看 THP 服务是否被禁用**
```shell
cat /sys/kernel/mm/transparent_hugepage/enabled
```
**如果返回的信息中显示的是以下结果，则表示 THP 服务没有被禁用**
	![](images/操作手册（重制版）-54.png)

​	**依次执行以下命令，将 THP 服务设置为禁用状态**
```shell
echo never > /sys/kernel/mm/transparent_hugepage/enabled
echo never > /sys/kernel/mm/transparent_hugepage/defrag
```
![](images/操作手册（重制版）-55.png)

​	**接下来，重新执行查看命令，再次检查 THP 服务的状态**
```shell
cat /sys/kernel/mm/transparent_hugepage/defrag
```
![](images/操作手册（重制版）-56.png)

### 任务活动2.1.4 阿里云yun源配置

1. **在所有节点上，执行以下命令，进入 “yum” 目录**
```shell
cd /etc/yum.repos.d/
```

2. **在所有节点上，执行以下命令，下载阿里云的 repo 文件**
```shell
wget http://mirrors.aliyun.com/repo/Centos-7.repo
```

3. **在所有节点上，执行以下命令，备份系统原来的 repo 文件**
```shell
mv CentOS-Base.repo CentOS-Base.repo.bak
```

4. **在所有节点上，执行以下命令，替换系统中原来的 repo 文件**
```shell
mv Centos-7.repo CentOS-Base.repo
```

5. **在所有节点上，执行以下命令， 执行 yum 源更新**
```shell
yum clean all
yum makecache
yum update
```

### 任务活动2.1.5 升级OpenSSL协议

1. **在所有节点上，依次执行以下命令，安装所需的编译器和工具**
```shell
yum group install -y 'Development Tools'
yum install -y perl-core zlib-devel
```

2. **下载 OpenSSL**
下载网站： https://github.com/openssl/openssl/tree/openssl-3.1

- **回到节点的连接窗口，执行以下命令，进入 “software” 命令**
```shell
cd /usr/software
```

​	**如果没有上面这个目录，就使用下面这个命令创建**
```shell
mkdir /usr/software
```

​	**解压命令**
```shell
unzip OpenSSL的文件名
```


- **git 方式下载**

  **执行以下命令，惊醒git工具的安装**

```shell
yum install -y git
```

​	**依次执行以下命令，从 GitHub 上下载最新的 OpenSSL**
```shell
# 如果当前目录已经是 software ， 这个进入目录的命令操作可以省略
cd /usr/software

# 执行 git 的克隆操作， 下载 openssl
git clone https://github.com/openssl/openssl.git
```

​	**克隆完成后的反馈信息如下图，执行 `ll` 命令进一步核对OpenSSL 的文件大小信息**
​	![](images/操作手册（重制版）-34.png)

3. **在所有节点上，依次执行以下命令，开始编译`Openssl`。 先进入`Openssl`解压后的目录，然后执行 “config" 命令**
```shell
# 如果选择的是 git 方式下载 OpenSSL, cd 命令后的目录名称，应该改为 openssl
cd openssl
config --prefix=/usr/software/ssl --openssldir=/usr/software/ssl shared zlib
```

​	**编译失败可查看下面的文章**
```shell
https://blog.csdn.net/sd4493091/article/details/122220902
https://blog.csdn.net/qq_41977843/article/details/125524691
```

​	**执行 “config" 命令后的成功效果**
​	![](images/操作手册（重制版）-35.png)

**继续执行 "make" 命令, 编译为二进制文件**
```shell
make
```
​	**执行 ”make” 成功后的截图**
​	![](images/操作手册（重制版）-36.png)

​	**继续执行 "make test" 命令， 进行测试**
```shell
make test
```

​	**继续执行 “make install" 命令， 进行安装**
```shell
make install
```
![](images/操作手册（重制版）-37.png)

4. **在所有节点上，依次执行以下命令， 配置Link Libraries**
```shell
cd /etc/ld.so.conf.d/
vi openssl.conf
```

​	**将下面内容写入到 `openssl.conf` 文件中**
```shell
/usr/software/ssl/lib64
```

5. **在所有节点上，执行以下命令， 重载动态Link**
```shell
ldconfig -v
```

​	**执行 `ldconfig` 命令成功后的效果**
​	![](images/操作手册（重制版）-38.png)

6. **在所有节点上，执行以下命令， 进行备份**
```shell
mv /usr/bin/openssl /usr/bin/openssl.backup
```

7. **在所有节点上，执行以下命令， 为`openssl`创建新的环境**
```shell
vi /etc/profile.d/openssl.sh
```

​	**将下面的内容写入 ”openssl.sh“ 文件中**
```shell
# Set OPENSSL_PATH
OPENSSL_PATH=/usr/software/ssl/bin
export OPENSSL_PATH
PATH=$PATH:$OPENSSL_PATH
export PATH
```

8. **在所有节点上，依次执行以下命令，使配置生效**
```shell
chmod -x /etc/profile.d/openssl.sh
source /etc/profile.d/openssl.sh
```

9. **在所有节点上，执行以下命令**
```shell
openssl version -a
```

​	**执行 ”version“ 命令成功后的效果如图**
​	![](images/操作手册（重制版）-40.png)

### 任务活动2.1.6 配置SSH免密登录

	**在所有节点上执行以下命令，生成密钥**
```shell
ssh-keygen
```

​	**执行 `ssh-keygen` 命令的操作过程如图**
​	![](images/操作手册（重制版）-41.png)

​	**在所有节点上， 进入 `.ssh` 目录**
```shell
cd /root/.ssh
```

​	**在所有节点上，依次执行以下命令**
```shell
ssh-copy-id -i id_rsa.pub root@zhanghoulin31-node1
ssh-copy-id -i id_rsa.pub root@zhanghoulin31-node2
ssh-copy-id -i id_rsa.pub root@zhanghoulin31-node3
```

​	**每次执行 `ssh-copy-id` 命令的操作过程**
​	![](images/操作手册（重制版）-42.png)

​	**执行完以上命令后，可以在集群中的某一节点上，进行SSH免密码登陆测试。如在节点 node2 上执行以下命令， 用SSH免密码方式登陆主节点 node1 **
```shell
ssh node1
```

​	**执行以上命令后，节点 node2 的命令行会跳转到节点 node1 成功登陆状态，退出SSH免密码登陆，可以用 “exit” ， 如图所示**
​	![](images/操作手册（重制版）-43.png)

### 任务活动2.1.7 集群时间同步配置

1. **安装 `chrony` 服务**
**执行以下命令， 查询是否安装有 `chrony` 服务**
```shell
yum list installed | grep chrony
```

​	**如果什么都没有返回，则执行以下命令，进行 `chrony` 服务的安装**

```shell
yum install -y chrony
```

2. **配置集群时间同步**
**首先在主节点上，打开 `chrony` 的配置文件 `chrony.conf`**
```shell
vi /etc/chrony.conf
```

​	**在文件，找到以下对应英文注释位置，根据中文注释要求，按如下代码格式添加对应配置信息**
```shell

# Use public servers from the pool.ntp.org project.
# Please consider joining the pool (http://www.pool.ntp.org/join.html).
server 0.centos.pool.ntp.org iburst
server 1.centos.pool.ntp.org iburst
server 2.centos.pool.ntp.org iburst
server 3.centos.pool.ntp.org iburst

# Record the rate at which the system clock gains/losses time.
driftfile /var/lib/chrony/drift

# Allow the system clock to be stepped in the first three updates
# if its offset is larger than 1 second.
makestep 1.0 3

# Enable kernel synchronization of the real-time clock (RTC).
rtcsync

# Enable hardware timestamping on all interfaces that support it.
#hwtimestamp *

# Increase the minimum number of selectable sources required to adjust
# the system clock.
#minsources 2

# Allow NTP client access from local network.
# 设置成自己网段，允许本局域网段的客户端访问
allow 192.168.40.2/24

# Serve time even if not synchronized to a time source.
# 本地时间服务器，即使时间服务器没有时间同步源，也可以作为时间服务器
local stratum 10

# Specify file containing keys for NTP authentication.
#keyfile /etc/chrony.keys

# Specify directory for log files.
logdir /var/log/chrony

# Select which information is logged.
#log measurements statistics tracking
```

​	**其次，在从节点 node2 和 node3 上， 分别执行以下命令，打开 `chrony.conf` 文件**
```shell
vi /etc/chrony.conf
```

​	**在文件中，找到以下内容，在这四个默认的时间同步服务器代码前加上 “#” 符号， 屏蔽掉他们**
**再次，在他们下面，另起一行，将主节点 node1 设置成集群中的时间同步服务端，添加后的效果如下**

```shell

# Use public servers from the pool.ntp.org project.
# Please consider joining the pool (http://www.pool.ntp.org/join.html).
# 屏蔽这四个默认的NTP服务器配置
# server 0.centos.pool.ntp.org iburst
# server 1.centos.pool.ntp.org iburst
# server 2.centos.pool.ntp.org iburst
# server 3.centos.pool.ntp.org iburst
# 将主节点 node1 设置成集群中的时间同步的服务器
server zhanghoulin31-node1 iburst

# Record the rate at which the system clock gains/losses time.
driftfile /var/lib/chrony/drift

# Allow the system clock to be stepped in the first three updates
# if its offset is larger than 1 second.
makestep 1.0 3

# Enable kernel synchronization of the real-time clock (RTC).
rtcsync

# Enable hardware timestamping on all interfaces that support it.
#hwtimestamp *

# Increase the minimum number of selectable sources required to adjust
# the system clock.
#minsources 2

# Allow NTP client access from local network.
#allow 192.168.0.0/16

# Serve time even if not synchronized to a time source.
#local stratum 10

# Specify file containing keys for NTP authentication.
#keyfile /etc/chrony.keys

# Specify directory for log files.
logdir /var/log/chrony

# Select which information is logged.
#log measurements statistics tracking
```

​	**在所有节点上， 执行一下命令， 启动 `chrony` 服务**
```shell
systemctl start chronyd
```

​	**在所有节点上， 执行以下命令，查看时间同步源的配置情况**
```shell
chronyc sources
```

​	**以下是主节点 node1 上应该反馈的信息，表明时间同步源的配置情况**
​	![](images/操作手册（重制版）-44.png)

​	**以下是两个从节点上都应该反馈的信息，表明主节点 node1 已经成为他们的时间同步服务器**
​	![](images/操作手册（重制版）-45.png)

​	**最后，在所有节点上， 执行以下命令，将 chronyd 设置成开机启动服务**
```shell
systemctl enable chronyd
```

​	**在所有节点上， 执行以下命令，从命令执行后返回的信息中，可以核对三个节点在时间上是否一致**
```shell
date
```

​	**在配置好 `chrony` 服务后， 各节点会在日后的操作中，随着虚拟机的频繁 “挂起” 操作，彼此之间再次出现时间不同步的情况，这很正常。遇到这种情况，可以在所有节点上，执行以下命令，重启 `chronyd` 服务，手动进行一次时间上的同步 **
```shell
systemctl restart chronyd
```

### 任务活动2.1.8 JDK的安装与配置

	**创建文件夹（如果已创建可以省略）**
```shell
mkdir /usr/software
```

​	**进入文件夹**
```shell
cd /usr/software
```

​	**使用 `wget` 下载 JDK 或者从物理机上上传到虚拟机**

```shell
wget https://repo.huaweicloud.com/java/jdk/8u202-b08/jdk-8u202-linux-x64.tar.gz
```

​	**创建 `/usr/java` 目录**

```shell
mkdir /usr/java
```

​	**将JDK解压到`/usr/java`目录中**

```shell
tar -zxf jdk-8u281-linux-x64.tar.gz -C /usr/java
```
![](images/操作手册（重制版）-46.png)

​	**创建软链接**
```
ln -s /usr/java/jdk-8u281-linux-x64/ /usr/java/jdk
```
![](images/操作手册（重制版）-47.png)

​	**修改环境配置文件**
```shell
vi /etc/profile
```

​	**向里面添加内容**
```shell
export JAVA_HOME=/usr/java/jdk
export JRE_HOME=/usr/java/jdk/jre
export $PATH=$JAVA_HOME/bin:$JRE_HOME:$PATH
export CLASSPATH=.:$JRE_HOME/lib/rt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/bin
```
![](images/操作手册（重制版）-48.png)

​	**重启环境变量，使之生效**
```shell
source /etc/profile
```

​	**查看 java 版本**
```shell
java -version
```

​	**结果如下**

![](images/操作手册（重制版）-49.png)

## 工作任务2.2 Hadoop集群安装与配置
### 任务活动2.2.1 在主节点上安装配置Hadoop

网页：[Apache Hadoop](https://hadoop.apache.org/releases.html)
打开网站，复制链接地址
![](images/操作手册（重制版）-58.png)
![](images/操作手册（重制版）-57.png)

- **回到 node1 节点的连接窗口，执行以下命令，进入 `software` 目录**
```shell
cd /usr/software
```

 - **执行以下命令，开始下载 `Hadoop` （注意：`wget`） 与下载地址之间有个英文空格，而且在下面代码中给出的下载地址只能作为格式参考，不能作为实际地址使用，实际地址请从官网界面上复制）。**
```shell
wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
```

 - **接下来，在 `/usr` 下创建新目录 `app`。**
```shell
mkdir /usr/app
```

 - **将 `Hadoop` 解压到新目录下 `app`**
```shell
tar -zxf hadoop-3.3.6.tar.gz -C /usr/app
```

 - **执行以下命令，进入 `Hadoop`解压后的`Hadoop`配置目录。**
```shell
cd /usr/app/hadoop-3.3.6/etc/hadoop/
```

 - **打开当前目录下的 `hadoop-env.sh`文件。**
```shell
vi hadoop-env.sh
```

 - **在文件头部，写入以下内容**
```shell
export JAVA_HOME=/usr/java/jdk
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
# hadoop 集群时间与系统时间保存同步
export HADOOP_OPTS="$HADOOP_OPTS -Duser.timezone=GMT+08"
```

 - **保存并关闭`hadoop-env.sh`文件后，执行以下命令，打开当前目录下的`core-site.xml`文件**
```shell
vi core-site.xml
```

 - **在`<configuration></configuration>`标签之间插入以下内容**
```xml
<!-- 指定 HDFS 的 nameservice -->
<property>
	<name>fs.defaultFS</name>
	<value>hdfs://zhanghoulin31-node1:9820</value>
</property>

<!-- 指定 Hadoop 缓存目录 -->
<property>
	<name>hadoop.tmp.dir</name>
	<value>/opt/hadoopdata</value>
</property>
```

 - **保存并关闭`core-site.xml`文件后，执行以下命令，打开当前目录下的`hdfs-site.xml`文件**
```shell
vi hdfs-site.xml
```

 - **在`<configuration></configuration>`标签之间插入以下内容。**
```xml
<property>
	<name>dfs.namenode.secondary.http-address</name>
	<value>zhanghoulin31-node2:9868</value>
</property>
```

 - **保存并关闭`hdfs-site.xml`文件后，执行一下命令，打开当前目录下的`workers`文。**
```shell
vi workers
```

 - **清空`workers`文件原来的内容，写入三个节点的主机名。**
```shell
zhanghoulin31-node1
zhanghoulin31-node2
zhanghoulin31-node3
```

 - **保存并关闭`workers`文件后，在所有节点上执行以下命令，在`/opt`目录下创建`hadoopdata`新目录**
```shell
mkdir /opt/hadoopdata
```

### 任务活动2.2.2 在从节点上安装配置Hadoop

- **在两个从节点`node2`、`node3`上，执行以下命令，创建新的目录`app`**
```shell
mkdir /usr/app
```

- **在主节点`node1`上，执行以下命令，进入`/usr/app`目录。**
```shell
cd /usr/app
```

- **将当前目录下的`Hadoop`复制到两个从节点`app`目录中**
```shell
scp -r hadoop-3.3.6 root@zhanghoulin-node2:$PWD
scp -r hadoop-3.3.6 root@zhanghoulin-node3:$PWD
```

- **在所有节点上，执行以下命令，关闭防火墙**
```shell
systemctl stop firewalld
```

- **在所有节点上，执行以下命令，将防火墙设置成开机不启动模式**
```shell
systemctl disable firewalld
```

### 任务活动2.2.3 格式化Hadoop的HDFS

- **在所有节点上执行以下命令，打开所有节点上的系统环境变量配置文件**
```shell
vi /etc/profile
```

- **在文件中，配置`Hadoop`环境变量**
```
export HADOOP_HOME=/usr/app/hadoop-3.3.6
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

- **在所有节点上执行以下命令，开始`NameNode`的格式化**
```shell
hdfs namenode -format
```

- **反馈信息中显示如有以下内容，表示格式化成功，如图所示**
	![](images/操作手册（重制版）-59.png)

### 任务验证

1. 启动与停止集群

![](images/操作手册（重制版）-69.png)

2. 查验WEB UI界面
![](images/操作手册（重制版）-65.png)

3. 验证HDFS文件系统是否可读写文件
	![](images/操作手册（重制版）-63.png)


## 工作任务2.3 Hadoop集群HDFS HA的安装与配置

组件HDFS通过脚本启动有三种方式，分别如下所述。
1. 方法一：HDFS组件的服务进程逐一启动或停止，命令格式如下：
```shell
hadoop-daemon.sh start|stop namenode|datanode|secondarynamenode
```
2. 方法二：HDFS组件的服务进程全部一次启动或停止，命令格式如下：
```shell
start-dfs.sh
stop-dfs.sh
```
3. 方法三：Hadoop的所有服务进程全部一次启动或停止
```shell
start-all.sh
stop-all.sh
```

我们经常采用第二种方法进行启停组件服务进程，在任务验证之前，我们自己编写一个脚本，方便在一台节点中查看所有集群的服务进程，首先在hadoop-node1节点中的/opt/目录下创建一个“bin/”目录，命令如下：
```shell
mkdir /opt/bin
```
然后通过“vi" 命令在bin目录下创建一个名为”jps-cluster.sh"的脚本文件，命令如下：
```shell
vi jps-cluster.sh
```
然后在脚本文件中输入如下代码内容后保存“jps-cluster.sh”文件。
```shell
#!/bin/bash
# 定义所有节点
HOSTS=(zhanghoulin31-node1 zhanghoulin31-node2 zhanghoulin31-node3)
# 遍历每个节点
for HOST in ${HOSTS[@]}
do
	# 远程登录到指定节点，执行jps命令
	ssh -T $HOST << TERMINATER
	echo "---------- $HOST ----------"
	jps | grep -iv jps
	exit
TERMINATER
done
```
接下来，在/opt/bin/目录下，为“jps-cluster.sh”增加执行权限，命令如下：
```shell
chmod a+x jps-cluster.sh
```
最后，在/etc/profile系统配置文件中，为当前的脚本文件配置系统环境变量，配置代码如下：
```shell
export PATH=$PATH:/opt/bin
```
配置完成后，执行如下命令让配置生效
```shell
source /etc/profile
```
接下来，我们进行HDFS组件的服务进程启停验证，执行以下命令：
```shell
start-dfs.sh
```
![](images/操作手册（重制版）-64.png)
执行"jps-cluster.sh"自编写的脚本文件，查看集群节点启动服务进程的情况如图
```shell
jps-cluster.sh
```
![](images/操作手册（重制版）-68.png)

### 任务活动2.3.1 Zookeeper的安装与配置

ZooKeeper网站：[Apache ZooKeeper](https://zookeeper.apache.org/releases.html)
	![](images/操作手册（重制版）-84.png)
	![](images/操作手册（重制版）-85.png)
进入主节点的“/usr/software”目录
```shell
cd /usr/app
```
下载ZooKeeper文件到当前目录
```
wget https://dlcdn.apache.org/zookeeper/zookeeper-3.9.1/apache-zookeeper-3.9.1-bin.tar.gz
```
执行以下命令，将zookeeper的安装包压缩文件复制到其他两个节点。
```shell
scp apache-zookeeper-3.9.1-bin.tar.gz root@zhanghoulin31-node2:/usr/software
scp apache-zookeeper-3.9.1-bin.tar.gz root@zhanghoulin31-node3:/usr/software
```
在所有节点上，执行以下命令，将各自的zookeeper解压到自己的/usr/app目录下。
```shell
tar -zxf apache-zookeeper-3.9.1-bin.tar.gz -C /usr/app
```
在主节点node1中，执行以下命令，进入Zookeeper的配置文件“conf” 。复制目录下的zoo_sample.cfg文件。
```shell
cp zoo_sample.cfg zoo.cfg
```
打开复制后的新文件zoo.cfg，将原来的dataDir=/tmp/zookeeper内容，修改为以下配置
```shell
dataDir=/opt/zookeeperdata
```
并追加写入以下内容。
```shell
server.1=zhanghoulin31-node1:2888:3888
server.2=zhanghoulin31-node2:2888:3888
server.3=zhanghoulin31-node3:2888:3888
```
保存并退出zoo.cfg文件后，执行以下命令，创建新目录"zookeeperdata"。
```shell
mkdir /opt/zookeeperdata
```
执行以下命令，在“zookeeperdata”目录下，创建新文件myid，并向myid文件中写入数字1。
```shell
echo 1 >> /opt/zookeeperdata/myid
```
在node2和node3节点中，也创建“zookeeperdata"新目录。在node2节点的”zookeeperdata"目录下，创建新文件myid，并写入数字2.
```shell
echo 2 >> /opt/zookeeperdata/myid
```
在node3节点的“zookeeperdata"目录下，创建新文件myid，并写入数字3
```shell
echo 3 >> /opt/zookeeperdata/myid
```
在主节点node1上，依次执行以下命令，将zookeeper配置目录下的zoo.cfg文件，复制到node2和node3节点对应的相同目录。
```shell
scp zoo.cfg root@zhanghoulin31-node2:$PWD
scp zoo.cfg root@zhanghoulin31-node3:$PWD
```
在所有节点上，执行以下命令，为zookeeper配置环境变量
```shell
vi /etc/profile
```
向文件中，追加写入
```shell
export ZOOKEEPER_HOME=/usr/app/apache-zookeeper-3.9.1-bin
export PATH=$PATH:$ZOOKEEPER_HOME/bin
```
在所有节点上，执行以下命令，使配置生效
```shell
source /etc/profile
```
在所有节点上，执行以下命令，启动zookeeper。
```shell
zkServer.sh start
```


### 任务活动2.3.2 HDFS HA(高可用)配置

配置之前停止所有的进程
```shell
zkServer.sh stop
```
进入主节点node1的hadoop配置目录
```shell
cd /usr/app/hadoop-3.3.6/etc/hadoop/
```
打开"hadoop-env.sh"文件
```shell
vi hadoop-env.sh
```
用#号屏蔽掉文件中的以下内容
```shell
# export HDFS_SECONDARYNAMENODE_USER=root
```
向文件中，追加新的内容
```shell
export HDFS_ZKFC_USER=root
export HDFS_JOURNALNODE_USER=root
```
保存并退出hadoop-env.sh文件后，打开同目录下的core-site.xml文件
```shell
vi core-site.xml
```
打开文件中的`<value>hdfs://node1:9820</value>`处，修改为`<value>hdfs://mycluster</value>`, 修改后的效果如下
```xml
<property>
	<name>fs.defaultFS</name>
	<value>hdfs://mycluster</value>
</property>
```
向文件中追加写入新的内容，注意添加的新内容必须写在`<configuration></configuration>`标签内。
```xml
<!-- 指定ZooKeeper集群地址 -->
<property>
	<name>ha.zookeeper.quorum</name>
	<value>zhanghoulin31-node1:2181,zhanghoulin31-node2:2181,zhanghoulin31-node3:2181</value>
</property>
```
保存并退出core-site.xml文件后，打卡hdfs-site.xml文件。
```shell
vi hdfs-site.xml
```
屏蔽掉以下内容
```xml
<property>
	<name>dfs.namenode.secondary.http-address</name>
	<value>zhanghoulin31-node2:9868</value>
</property>
```
追加写入以下新的内容。写在`<configuration></configuration>`标签内
```xml
<configuration>
	<!-- 设置副本个数 -->
	<property>
		<name>dfs.replication</name>
		<value>2</value>
	</property>
	
	<!-- 设置namenode.name目录 -->
	<property>
		<name>dfs.namenode.name.dir</name>
		<value>file:/opt/hadoopdata/name</value>
	</property>
	
	<!-- 设置datanode.data目录 -->
	<property>
		<name>dfs.datanode.data.dir</name>
		<value>file:/opt/hadoopdata/data</value>
	</property>
	
	<!-- 开启Web HDFS -->
	<property>
		<name>dfs.webhdfs.enabled</name>
		<value>true</value>
	</property>
	
	<!-- 指定HDFS的nameservice,与core-site.xml中保持一致 -->
	<property>
		<name>dfs.nameservices</name>
		<value>mycluster</value>
	</property>
	
	<!-- 指定HDFS的nameservice下，由两个NameNode，分别是nn1和nn2 -->
	<property>
		<name>dfs.ha.namenodes.mycluster</name>
		<value>nn1,nn2</value>
	</property>
	
	<!-- 设置nn1的RPC通信地址 -->
	<property>
		<name>dfs.namenode.rpc-address.mycluster.nn1</name>
		<value>zhanghoulin31-node1:9820</value>
	</property>
	
	<!-- 设置nn2的RPC通信地址 -->
	<property>
		<name>dfs.namenode.rpc-address.mycluster.nn2</name>
		<value>zhanghoulin31-node2:9820</value>
	</property>
	
	<!-- 设置nn1的http通信地址-->
	<property>
		<name>dfs.namenode.http-address.mycluster.nn1</name>
		<value>zhanghoulin31-node1:9870</value>
	</property>
	
	<!-- 设置nn2的http通信地址 -->
	<property>
		<name>dfs.namenode.http-address.mycluster.nn2</name>
		<value>zhanghoulin31-node2:9870</value>
	</property>
	
	<!-- 指定NameNode的元数据在JournalNode上的存放位置 -->
	<property>
		<name>dfs.journalnode.edits.dir</name>
		<value>/opt/journaldata/data</value>
	</property>
	
	<!--  -->
	<property>
		<name>dfs.namenode.shared.edits.dir</name>
		<value>qjournal://zhanghoulin31-node1:8485;zhanghoulin31-node2:8485;zhanghoulin31-node3:8485/mycluster</value>
	</property>
	
	<!-- 指定JournalNode在本地磁盘存放数据的位置 -->
	<property>
		<name>dfs.journalnode.edits.dir</name>
		<value>/opt/journalnode/data</value>
	</property>
	
	<!-- 设置失败时自动切换实现方式 -->
	<property>
		<name>dfs.client.failover.proxy.provider.mycluster</name>
		<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
	</property>
	
	<!-- 设置隔离机制方法，多个机制用换行分割，即每个机制暂用一行 -->
	<property>
		<name>dfs.ha.fencing.methods</name>
		<value>
			sshfence
			shell(/bin/true)
		</value>
	</property>
	
	<!-- 使用sshfence隔离机制时需要ssh免登录 -->
	<property>
		<name>dfs.ha.fencing.ssh.private-key-files</name>
		<value>/root/.ssh/id_rsa</value>
	</property>
	
	<!-- 开启NameNode失败自动切换 -->
	<property>
		<name>dfs.ha.automatic-failover.enabled</name>
		<value>true</value>
	</property>
	
	<!-- 设置sshfence隔离机制超时时间 -->
	<property>
		<name>dfs.ha.fencing.ssh.connect-timeout</name>
		<value>30000</value>
	</property>
	
	<!-- 关闭权限检查 -->
	<property>
		<name>dfs.permissions.enabled</name>
		<value>false</value>
	</property>
</configuration>
```
在所有节点上，执行以下命令，在/opt目录下，创建二级新子目录journalnode/data。
```shell
mkdir -p /opt/journalnode/data
```
在主节点node1上，执行以下命令，将主节点node1的这三个刚刚修改好的配置文件复制到node2、node3节点相同的hadoop配置目录，覆盖掉他们目录下原来的文件
```shell
scp hadoop-env.sh core-site.xml hdfs-site.xml root@zhanghoulin31-node2:$PWD
scp hadoop-env.sh core-site.xml hdfs-site.xml root@zhanghoulin31-node3:$PWD
```
- **启动JN节点**
在所有节点上，执行以下命令，启动zookeeper。
```shell
zkServer.sh start
```
在所有节点上，执行以下命令，启动监控NameNode管理日志的journalnode，简称JN。
```shell
hdfs --daemon start journalnode
```
在所有节点上，执行以下命令，查看进程运行情况
```shell
jps
```
反馈信息中显示有JouranlNode进程， 表示JN节点运行成功
```shell
Jps
QuorumPeerMain
JournalNode
```

### 任务活动2.3.3 NameNode 与 ZKFC格式化

在主节点node1上，执行以下命令，再次对主节点node1的namenode进行格式化。
```shell
hdfs namenode -format
```
反馈信息中如显示有以下内容，表示格式化成功。
![](images/操作手册（重制版）-70.png)
1. [如果你正在使用Hadoop HA模式并且遇到了问题，你可能需要先启动所有的ZooKeeper和JournalNode进程，然后再进行格式化](https://zhuanlan.zhihu.com/p/65764220)。
2. 但是由于目录`/opt/journalnode/data/mycluster`不为空，**需要在所有节点上把这个目录删除**
```shell
rm -rf /opt/journalnode/data/mycluster/*
```
![](images/操作手册（重制版）-69.png)
进入主节点node1的“/opt”目录
```shell
cd /opt
```
将主节点node1的元数据复制到node2，node3节点的/opt目录下
```shell
scp -r hadoopdata root@zhanghoulin31-node2:$PWD
scp -r hadoopdata root@zhanghoulin31-node3:$PWD
```
在主节点node1上，执行以下命令，开始格式化ZKFC。
```shell
hdfs zkfc -formatZK
```
 当反馈信息中出现如下信息时 ，在末尾输入“y“,回车确认。(我没有)
	![](images/操作手册（重制版）-71.png)
没有成功，查看zookeeper是否启动成功
```shell
zkServer.sh status
```
在所有节点上，可以执行以下命令，退出JournalNode进程
```shell
hdfs --daemon stop journalnode
```

- **启动Hadoop的HA集群**
在所有节点上， 启动zookeeper。
```shell
zkServer.sh start
```
在主节点node1上，执行以下命令，启动集群。
```shell
start-dfs.sh
```
hadoop集群正常启动后，在浏览器中分别输入”http://node1:9870“和”http://node2:9870“地址，打开节点node1和节点node2的HDFS Web页面，node1节点的状态时active（活跃）状态，node2节点目前的状态时standby（备用）状态。
	![](images/操作手册（重制版）-74.png)
	![](images/操作手册（重制版）-73.png)
输入jps查看各节点的进程运行情况
node1
![](images/操作手册（重制版）-79.png)
node2
![](images/操作手册（重制版）-80.png)
node3
![](images/操作手册（重制版）-81.png)






# 项目三 Hadoop YARN 高可用集群搭建与维护
## 工作任务 3.1 Hadoop 集群 YARN HA 搭建
### 任务活动3.1.1 YARN 的HA搭建配置

##### 1 什么是YARN
	YARN是Hadoop 框架自带的一个功能模块

##### 2 YARN的HA搭建
###### 2.1 编辑hadoop-env.sh文件

- 进入主节点node1的Hadoop安装路径下的“/etc/hadoop“目录，打开目录下hadoop-env.sh文件/
```shell
cd /usr/app/hadoop-3.3.6/etc/hadoop
vi hadoop-env.sh
```
- 在hadoop-env.sh文件的首行前，添加以下内容
```shell
export YARN_NODEMANAGER_USER=root
export YARN_RESOURCEMANAGER_USER=root
```
###### 2.2 编辑mapred-site.xml文件
- 打开同目录下的mapred-site.xml文件。
```shell
vi mapred-site.xml
```
- 在<configuration\></configuration\>标签之间插入以下内容。
```xml
<!-- 指定MapReduce框架为YARN方式 -->
<property>
	<name>mapreduce.framework.name</name>
	<value>yarn</value>
</property>

<!-- 配置hadoop_mapred_home, 不配MapReduce离线框架任务job.wordcount程序报错 -->
<property>
	<name>yarn.app.mapreduce.am.env</name>
	<value>HADOOP_MAPRED_HOME=/usr/app/hadoop-3.3.6</value>
</property>

<property>
	<name>mapreduce.map.env</name>
	<value>HADOOP_MAPRED_HOME=/usr/app/hadoop-3.3.6</value>
</property>

<property>
	<name>mapreduce.reduce.env</name>
	<value>HADOOP_MAPRED_HOME=/usr/app/hadoop-3.3.6</value>
</property>

<!-- 历史服务配置，不配，yarn上看不到日志信息 -->
<property>
	<name>mapreduce.jobhistory.intermediate-done-dir</name>
	<value>/mr-history/tmp</value>
</property>

<property>
	<name>mapreduce.jobhistory.cleaner.enable</name>
	<value>true</value>
</property>

<property>
	<name>mapreduce.jobhistory.cleaner.interval-ms</name>
	<value>86400000</value>
</property>
```

###### 2.3 编辑yarn-site.xml文件
- 打开同目录下的yarn-site.xml文件
```shell
vi yarn-site.xml
```
- 在<configuration\></configuration\>标签之间插入以下内容。
```xml
<!-- 设置nodemanager的缓存大小 -->
<property>
	<name>yarn.nodemanager.resource.memory-mb</name>
	<value>2048</value>
</property>

<!-- 设置scheduler的缓存值上限 -->
<property>
	<name>yarn.scheduler.maximum-allocation-mb</name>
	<value>2048</value>
</property>

<!-- 设置nodemanager的CPU虚拟内核数 -->
<property>
	<name>yarn.nodemanager.resource.cpu-vcores</name>
	<value>1</value>
</property>

<!-- 设置NodeManager启动时加载Shuffle服务 -->
<property>
	<name>yarn.nodemanager.aux-services</name>
	<value>mapreduce_shuffle</value>
</property>

<!-- 启动yarn ResourceManager 的HA模式 -->
<property>
	<name>yarn.resourcemanager.ha.enabled</name>
	<value>true</value>
</property>

<!-- 设置yarn ResourceManager的集群ID -->
<property>
	<name>yarn.resourcemanager.cluster-id</name>
	<value>yrc</value>
</property>

<!-- 指定yarn ResourfeManager 实现HA的节点名称 -->
<property>
	<name>yarn.resourcemanager.ha.rm-ids</name>
	<value>rm1,rm2</value>
</property>

<!-- 设置启动rm1的主机为节点机node1 -->
<property>
	<name>yarn.resourcemanager.hostname.rm1</name>
	<value>zhanghoulin31-node1</value>
</property>

<!-- 设置启动rm2的主机为节点机node2 -->
<property>
	<name>yarn.resourcemanager.hostname.rm2</name>
	<value>zhanghoulin31-node2</value>
</property>

<!-- 设置rm1的Web地址 -->
<property>
	<name>yarn.resourcemanager.webapp.address.rm1</name>
	<value>zhanghoulin31-node1:8088</value>
</property>

<!-- 设置rm2的Web地址 -->
<property>
	<name>yarn.resourcemanager.webapp.address.rm2</name>
	<value>zhanghoulin31-node2:8088</value>
</property>

<!-- 设置ZooKeeper集群地址，集群间的协调管理离不开ZooKeeper配置 -->
<property>
	<name>yarn.resourcemanager.zk-address</name>
	<value>zhanghoulin31-node1:2181,zhanghoulin31-node2:2181,zhanghoulin31-node3:2181</value>
</property>


```

###### 2.4 编辑yarn-env.sh文件
- 打开同目录下的yarn-env.sh文件
```shell
vi yarn-env.sh
```
- 在文件中追加
```shell
HADOOP_OPTS="$HAOOP_OPTS -Duser.timezone=GMT+08"
```

###### 2.5 其他从节点的配置
- 依次执行以下命令，将主节点node1的hadoop-env.sh、mapred-site.xml、yarn-env.sh和yarn-site.xml这四个文件复制到其他两个从节点中
```shell
scp hadoop-env.sh mapred-site.xml yarn-env.sh yarn-site.xml root@zhanghoulin31-node2:$PWD
scp hadoop-env.sh mapred-site.xml yarn-env.sh yarn-site.xml root@zhanghoulin31-node3:$PWD
```

#### 3.1 启动集群
- 在所有节点上，启动zookeeper。
```shell
zkServer.sh start
```
- 在主节点node1上，执行以下命令，启动YARN。
```shell
start-yarn.sh
```
- 在浏览器中分别输入”http://node1:8088“和”http://node2:8088“，打开两个节点的YARN WEB页面，单击页面左边的”About“, 会看到node1节点的YARN当前状态，目前时active, node2节点目前时standby状态。
	![](images/操作手册（重制版）-78.png)
	![](images/操作手册（重制版）-90.png)

- 在node1节点查进程运行情况，会有以下进程正在运行。
	![](images/操作手册（重制版）-82.png)
- 在node2节点查进程运行情况，会有以下进程正在运行。
	![](images/操作手册（重制版）-80.png)
- 在node3节点查进程运行情况，会有以下进程正在运行。
	![](images/操作手册（重制版）-81.png)

#### 3.2 启动历史查看服务器
- 在所有节点上按顺序执行以下两个命令，可以开启job的历史查看服务。
```shell
mapred --daemon start historyserver
yarn --daemon start timelineserver
```
- 开启后，用”jps“查看进程，如果返回的进程信息中出现以下这两个进程名，表示服务启动成功。
	![](images/操作手册（重制版）-89.png)

- 其他命令
```shell
yarn --daemon stop timelineserver
mapred --daemon stop historyserver

stop-yarn.sh
zkServer.sh
```

- 测试：mapreduce 程序是否能运行成功。补，非常重要。
- 先上传一个带单词的txt文本文档。然后执行以下命令（补）
```shell
hadoop jar /usr/app/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /usr/root/input/ /user/root/output4/
```

在YARN的WEB页面，可以打开”History"链接，查看在YARN集群中，各job的历史运行记录（注：只有提交了job任务，YARN的Web页面中才会有job的历史运行记录）。掌握job历史运行记录的查看，对调试基于MapReduce的开发项目（如：Hive、Pig等）非常有帮助。

点开“History”链接，页面的端口号是“19888”， 可以单击页面上的“logs”链接，查看job的运行日志，一旦job运行失败，从日志的信息中，可以帮助开发人员分析job失败的具体原因。
```shell
hadoop jar /usr/app/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar wordcout /user/root/input/ /user/root/output8/
```
![](images/操作手册（重制版）-91.png)


#### 3.3 操作 HDFS

##### 3.3.1 命令方式
操作前，确保 ZooKeeper 和 Hadoop 已启动。集群启动后，以下命令的操作可以在集群中的任意一节点上操作完成。
1. 查询目录命令 “-ls”
- 语法格式：
```
hdfs dfs -ls 要查询的 HDFS 目录
```
- 例如：以下操作查看 HDFS 根目录下的内容
```
hdfs dfs -ls /
```

2. 新建目录命令 ”-mkdir”
- 语法格式：
```
hdfs dfs -mkdir [-p] 要创建的 hdfs 新目录名称
```
“-p”: 当一次创建多级新目录时，需要添加/
- 例如：以下操作，实现在 HDFS 的根目录下创建两级新子目录 “test/01”。
```
hdfs dfs -mkdir /test/01
```

3. 删除目录命令 “-rmdir”
- 语法格式：
```
hdfs dfs -rmdir 要删除的 hdfs 目录名称
```
- 例如：以下操作，实现 HDFS 目录中 ”01“ 子目录的删除。
```
hdfs dfs -rmdir /test/01
```

4. 上传文件命令 ”-put“
- 语法格式：
```
hdfs dfs -put [-f] 相对或绝对路径 / 本地文件名称 上传到 HDFS的绝对路径名称
```
”-f“: 当需要替换 HDFS 目录下的同名文件时添加
- 例如： 以下操作，实现将当前目录下的 ”test.txt" 文件上传到 HDFS 的 “test.txt" 目录， ”-f" 表示要覆盖上传模式。
```
hdfs dfs -put test.txt /test_data
```

5. 下载文件命令 “-get”
- 语法格式：
```
hdfs dfs -get 要下载的 hdfs 绝对路径文件名称 本地绝对或相对路径
```
- 例如：以下操作，实现将 HDFS 中的 “person4.txt” 文件下载到当前本地目录，前提时 “person4.txt” 文件已存在与 HDFS 的 ”/test_data“ 目录下。
```
hdfs dfs -get /test_data/person4.txt 
```

6. 删除文件命令 “-rm”
- 语法格式：
```
hdfs dfs -rm [-f] [-r] 要删除 HDFS 绝对路径下的文件名
```
”-f“：当不需要确认，直接强制删除时添加。
”-r“：当需要一次性删除同一目录下的多个文件时添加。
文件名：可以使用 ”*“（星号）通配符，通常与 ”-r“ 参数一起配合使用。
- 例如：以下操作，实现删除 HDFS 中的 ”person4.txt“ 文件
```
hdfs dfs -rm /test_data/person4.txt
```
如果想了解更多 HDFS 命令，可通过 ”-help“ 查询。
```
hdfs dfs -help
```

##### 3.3.2 编程方式
除命令方式外，也可以通过 Java 编程方式实现 HDFS 的读写、添加、删除等操作。编程方式相对于命令方式，步骤多，难度大，但处理数据则更加灵活，能满足复杂情况下的一些处理需求。本小节以最常用到的写操作为例，实现一个 HDFS 文件写操作项目。
该项目特点：
1. 读取本地文件中的 JSON 格式数据。
2. 将读取的 JSON 格式数据转成实体类对象数据。
3. 对实体类对象数据进行过滤清洗，保留需求的数据。
4. 将保留下的数据写入 HDFS。
项目的完整结构如下图。


注：图中的 ”log4j.properties“ 文件从已安装的 Hadoop 目录下的 ”/etc/hadoop“ 目录中拷贝而来。
项目中的两个 Java 文件，各自的作用分别如下：
1.  ”OrdersInfo.java“ 文件：JavaBean 实体类，用于数据格式的转换。
2. ”App.java“文件：项目的执行入口文件，完成数据的读和写。


1. **创建项目**
在 eclipse 工具的菜单栏，依次单击 ”File" -> "New" -> "Project", 在弹出的 ”New Project“ 窗口中，选择 ”Maven“ 下的 ”Maven Project“ 选项，然后单击 "Next" 按钮， 进入下一步。但在创建 Maven Project 项目前，应先进行 Maven 本地化环境的配置，具体配置步骤参考附录一 ”Windows 环境下 Maven 工程开发环境的安装与配置“ 中的内容。
	![](images/操作手册（重制版）-92.png)
在弹出的 ”New Maven Project“ 窗口中，勾选第一项，单击 ”Next“ 按钮，进入下一步。
	![](images/操作手册（重制版）-93.png)
在弹出的新窗口中， 在 ”Group Id“ 处，填写倒序的公司和组织域名；在 ”Artifact Id“ 除填写模块名（工程名）； ”Version“ 为版本号，选择默认（这三个属性可以在 Maven 仓库中唯一定位一个 Maven 工程）。单击 ”Finish“ 按钮，完成工程的创建。



2. **源代码部分**
"pom.xml" 文件中的内容
```xml
<project xmlns="http://maven.apache.org/POM/4.0.0"  
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"  
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <groupId>org.my.hdfs</groupId>
    <artifactId>hdfs-write</artifactId>
    <version>0.0.1-SNAPSHOT</version>
  
    <dependencies>
        <dependency>
			<groupId>org.apache.hadoop</groupId>
			<artifactId>hadoop-client</artifactId>
            <version>3.3.1</version>
        </dependency>
        <dependency>
	        <groupId>org.slf4j</groupId>
            <artifactId>slf4j-simple</artifactId>
            <version>1.7.25</version>
            <scope>compile</scope>
        </dependency>
        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>fastjson</artifactId>
            <version>1.2.41</version>
            <scope>compile</scope>
        </dependency>
	</dependencies>
    <build>
		<plugins>
            <plugin>
	            <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>2.5</version>
                <configuration>
	                <source>1.8</source>
                    <target>1.8</target>
                    <encoding>UTF-8</encoding>
                </configuration>
			</plugin>
		<plugin>
			<groupId>org.apache.maven.plugins</groupId>
			<artifactId>maven-shade-plugin</artifactId>
			<version>2.3</version>
			<configuration>
				<transformers>
					<transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
						<mainClass>org.my.hdfs.App</mainClass>
					</transformer>
				</transformers>
			</configuration>
				<executions>
					<execution>
						<phase>package</phase>
                        <goals>
	                        <goal>shade</goal>
                        </goals>
					</execution>
				</executions>
			</plugin>
			<plugin>
				<artifactId>maven-assembly-plugin</artifactId>
                <configuration>
	                <descriptorRefs>
		                <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                    <archive>
						<manifest>
							<mainClass>org.my.hdfs.App</mainClass>
                        </manifest>
					</archive>
				</configuration>
				<executions>
					<execution>
                        <id>make-assembly</id>
                        <phase>package</phase>
                        <goals>
	                        <goal>single</goal>
                        </goals>
					</execution>
				</executions>
			</plugin>
		</plugins>
	</build>
</project>
```

"OrdersInfo.java" 文件中的内容
```java
package org.my.hdfs;  
  
import java.util.Date;  
  
public class OrdersInfo {  
    // 订单ID  
    private String orderId;  
    // 下单时间  
    private long createOrderTime;  
    // 支付编号  
    private String paymentId;  
    // 支付时间  
    private Date paymentTime;  
    // 商品ID  
    private String goodsId;  
    // 商品名  
    private String goodsName;  
    // 价格  
    private float goodsPrice;  
    // 价格折扣  
    private float promotionRate;  
    // 店铺ID  
    private String shopId;  
    // 店铺名  
    private String shopName;  
    // 店铺电话  
    private String shopMobile;  
    // 单笔支付总价  
    private float payPrice;  
    // 单笔订购数量  
    private int num;  
    // 省  
    private String province;  
    // 市  
    private String city;  
    // 县  
    private String county;  
    // 类别  
    private String catagorys;  
  
    public String getOrderId() {  
        return orderId;  
    }  
  
    public void setOrderId(String orderId) {  
        this.orderId = orderId;  
    }  
  
    public long getCreateOrderTime() {  
        return createOrderTime;  
    }  
  
    public void setCreateOrderTime(long createOrderTime) {  
        this.createOrderTime = createOrderTime;  
    }  
  
    public String getPaymentId() {  
        return paymentId;  
    }  
  
    public void setPaymentId(String paymentId) {  
        this.paymentId = paymentId;  
    }  
  
    public Date getPaymentTime() {  
        return paymentTime;  
    }  
  
    public void setPaymentTime(Date paymentTime) {  
        this.paymentTime = paymentTime;  
    }  
  
    public String getGoodsId() {  
        return goodsId;  
    }  
  
    public void setGoodsId(String goodsId) {  
        this.goodsId = goodsId;  
    }  
  
    public String getGoodsName() {  
        return goodsName;  
    }  
  
    public void setGoodsName(String goodsName) {  
        this.goodsName = goodsName;  
    }  
  
    public float getGoodsPrice() {  
        return goodsPrice;  
    }  
  
    public void setGoodsPrice(float goodsPrice) {  
        this.goodsPrice = goodsPrice;  
    }  
  
    public float getPromotionRate() {  
        return promotionRate;  
    }  
  
    public void setPromotionRate(float promotionRate) {  
        this.promotionRate = promotionRate;  
    }  
  
    public String getShopId() {  
        return shopId;  
    }  
  
    public void setShopId(String shopId) {  
        this.shopId = shopId;  
    }  
  
    public String getShopName() {  
        return shopName;  
    }  
  
    public void setShopName(String shopName) {  
        this.shopName = shopName;  
    }  
  
    public String getShopMobile() {  
        return shopMobile;  
    }  
  
    public void setShopMobile(String shopMobile) {  
        this.shopMobile = shopMobile;  
    }  
  
    public float getPayPrice() {  
        return payPrice;  
    }  
  
    public void setPayPrice(float payPrice) {  
        this.payPrice = payPrice;  
    }  
  
    public int getNum() {  
        return num;  
    }  
  
    public void setNum(int num) {  
        this.num = num;  
    }  
  
    public String getProvince() {  
        return province;  
    }  
  
    public void setProvince(String province) {  
        this.province = province;  
    }  
  
    public String getCity() {  
        return city;  
    }  
  
    public void setCity(String city) {  
        this.city = city;  
    }  
  
    public String getCounty() {  
        return county;  
    }  
  
    public void setCounty(String county) {  
        this.county = county;  
    }  
  
    public String getCatagorys() {  
        return catagorys;  
    }  
  
    public void setCatagorys(String catagorys) {  
        this.catagorys = catagorys;  
    }  
  
    @Override  
    public String toString() {  
        return "PaymentInfo{" +  
                "orderId='" + this.orderId + '\'' +  
                ", createOrderTime=" + this.createOrderTime +  
                ", paymentId='" + this.paymentId + '\'' +  
                ", paymentTime=" + this.paymentTime +  
                ", goodsId='" + this.goodsId + '\'' +  
                ", goodsName='" + this.goodsName + '\'' +  
                ", goodsPrice=" + this.goodsPrice +  
                ", promotionRate=" + this.promotionRate +  
                ", shopId='" + this.shopId + '\'' +  
                ", shopName='" + this.shopName + '\'' +  
                ", shopMobile='" + this.shopMobile + '\'' +  
                ", payPrice=" + this.payPrice +  
                ", num=" + this.num +  
                "}";  
    }  
}
```

"App.java" 文件中的内容。
```java
package org.my.hdfs;  
  
import java.io.File;  
import java.io.IOException;  
import java.util.ArrayList;  
import java.util.List;
import org.my.hdfs.OrdersInfo;
import org.apache.commons.io.FileUtils;  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.FSDataOutputStream;  
import org.apache.hadoop.fs.FileSystem;  
import org.apache.hadoop.fs.Path;
import com.alibaba.fastjson.JSONObject;

public class App {
    // 用于存储从日志文件读取的商品对象
    private List<OrdersInfo> m_list;

    public App() {
        this.m_list = new ArrayList<OrdersInfo>();
    }

    // 对本地文件的读操作
    public void readFile(String path) {
        try {
            // 创建文件对象
            File file = FileUtils.getFile(path);
            // 从文件中读取全部数据
            List<String> lines = FileUtils.readLines(file, "utf-8");
            // 对数据进行迭代
            for (String line : lines) {
                // 将读取的每行数据转成商品对象
                OrdersInfo osi = JSONObject.parseObject(line, OrdersInfo.class);
                // 存入列表中
                this.m_list.add(osi);
            }
        } catch (IOException ie) {
            ie.printStackTrace();
        }
    }

    // 对 HDFS 的写操作
    public void writeToHDFS(String path) {
        // 创建 HDFS 配置对象
        Configuration configuration = new Configuration();
        // 设置 active 状态下的 Hadoop 节点信息
        configuration.set("fs.defaultFS", "hdfs://node1:9820");
        // 设置 HDFS 信息
        configuration.set("fs.hdfs.impl", "org.apache.hadoop.hdfs.DistributedFileSystem");
        // 设置用户信息
        System.setProperty("HADOOP_USER_NAME", "root");

        try {
            // 创建 HDFS 的文件系统对象
            FileSystem fs = FileSystem.get(configuration);
            // 创建 HDFS 的路径对象
            Path p = new Path(path);
            // 检测提交的路径是否是绝对路径
            if (p.isAbsolute() == false)
                p = new Path("/" + path);
            // 创建 HDFS 数据流写对象，设置覆盖写为 true， 缓存大小 1024 字节
            FSDataOutputStream fos = fs.create(p, true, 1024);
            // 从列表中读取商品对象
            for (OrdersInfo osi : this.m_list) {
                // 对商品信息进行过滤，保留符合条件的商品
                if (osi.getPayPrice() >= 300.0) {
                    // 对商品信息进行清洗，按需求重新组装格式。
                    // 商品名与销售额之间用英文冒号连接，
                    // 每个对象信息之间用英文的空格连接，采用字节格式写入 HDFS。
                    fos.write((osi.getShopName() + "-" + osi.getGoodsName() + ":" + osi.getPayPrice() + " ").getBytes());
                }
            }
            fos.close();
        } catch (IOException ie) {
            ie.printStackTrace();
        }
    }

    public static void main(String[] args) {
    	// TODO Auto-generated method stub  
		// for (String aString : args) {
		//     System.out.println(aString);
		// }

        if (args.length >= 2) {
            App app = new App();
            // 读 / 写操作的文件位置，通过参数提交
            app.readFile(args[0]);
            app.writeToHDFS(args[1]);
        } else {
            System.out.println("缺少必要的参数！");
        }
    }
}

```

3. **操作部分**
在 eclipse 工具的 ”Project Explorer“ 或 ”Package Explorer“ 窗口，选择工程名称，单击鼠标右键，在快捷菜单中依次选择 ”Show in local Terminal“ -> "mvn" 选项，打开一个 ”Terminal" 窗口。
注意：整个编译过程中，计算机必须联通网络，且网速不能太慢（因为在实践教学过程中发现，有些学员会使用计算机手机流量上网，而手机流量网速慢，往往造成项目的 jar 包编译失败）。
	![](images/操作手册（重制版）-94.png)

在 “Terminal” 窗口，如果能显示下图所示的信息，则表明 jar 包文件编译成功。
	![](images/操作手册（重制版）-95.png)

刷新工程下的 “target” 文件夹，会看到此目录下出现编译后的 “jar” 文件。
	![](images/操作手册（重制版）-96.png)
将编译好的项目 “jar" 文件和本章提供的素材文件 ”orderinfo.log“， 以 secureFX 方式导入主节点 node1 的 ”/usr/test“ 目录下。 ”orderinfo.log“ 是一个关于商品交易的日志文件，其数据格式为 json 格式， 数据结构参见项目中的 ”order info.java“文件代码注释，日志文件中的部分数据示例如下。
```json
{"catagorys": "4000, 9900, 6400","city": "36","county": "274","createOrderTime": 1607702482000,"goodsName": "毛呢大衣","goodsPrice": 2600.0,"num": 5,"orderId": "2475ae37623a4f6ba5693df8e314fla4","payPrice": 8449.999,"paymentId": "a66fe77088504955baed84544d2","promotionRate": 0.65,"province": "4","shopId": "152000","shopName": "茗思"}
{"catagorys": "7900, 1500, 2400","city": "170","county": "588","createOrderTime": 1607702499000,"goodsName": "短外套","goodsPrice": 480.0,"num": 7,"orderId": "2475ae37623a4f1234693df8e314fla4","payPrice": 3024.0,"paymentId": "a66fe77088512345baed84544d2","promotionRate": 0.9,"province": "10","shopId": "145000","shopName": "Mongyi"}
{"catagorys": "8300, 300, 5300","city": "31","county": "1435","createOrderTime": 1607702406000,"goodsName": "半身裙","goodsPrice": 280.0,"num": 9,"orderId": "2475ae3762312345a5693df8e314fla4","payPrice": 1764.0,"paymentId": "a66fe77012504955baed84544d2","promotionRate": 0.7,"province": "17","shopId": "2000","shopName": "ZARA"}
```
启动好 ZooKeeper 集群 + Hadoop 集群后，检查主节点 node1 是否是 active 状态，必须保证主节点 node1 是 active 状态。 然后在主节点 node1 的连接窗口，执行以下命令，进入 ”test" 目录。
```
cd /usr/test
```
在 “test" 目录下，执行以下命令，运行项目的 ”jar" 文件，对同一目录下的商品日志文件 “orderinfo.log” 中的数据进行清洗，并将清洗的结果保存到 HDFS 的“/test-write-hdfs/data.txt" 文件中，注意以下命令中 ”orderinfo.log“ 与 ”/test-write-hdfs/data.txt“ 之间有英文的空格间隔。
```
java -jar hdfs-write-0.0.1-SNAPSHOT-jar-with-dependencies.jar orderinfo.log /test-write-hdfs/data.txt
```
项目 jar 文件运行后， 会反馈以下信息，可以忽略。
	![](images/操作手册（重制版）-97.png)
打开浏览器，地址栏输入 HDFS 的地址 ”node1 的主机名：9870" 后， 按回车键。如图，在页面的菜单栏，单击 “Utlities” -> "Browse the file system"， 使页面转到 “Browse Directiry” 页面。
	![](images/操作手册（重制版）-98.png)
在 “Browse Directiry” 页面的输入框（如图）输入 HDFS 的根目录 “/” 后，单击 “Go!” 按钮，页面中会显示当前 HDFS 根目录下的所有子目录。
	![](images/操作手册（重制版）-99.png)
如下图， 访问 “/test-write-hdfs” 子目录， 清洗后的数据结果存在与此目录下的 data.txt 文件中。
![](images/操作手册（重制版）-100.png)
如下图， 单击 “data.txt” 文件， 在弹出的窗口中， 单击上方中间的 “Head the file (first 32k)” 菜单，在窗口的下方文本域会显示 “data.txt" 文件中前 32k 大小的文件内容。
	![](images/操作手册（重制版）-101.png)


# 项目四 分布式数据库H Base
## 任务4.1 HBase的HA搭建
### 1 什么是HBase

Hbase是Hadoop生态圈中一个开源、分布式、非关系型的数据库。以数据为中心、面向列存储，允许不同类型的数据放在一起。HBase数据表有Column Family(列簇)、Column(列)、Row Key(行健) 和 Row(行)组成的。族是列的一个集合，族可以包含任意数量的列，列族必须是在创建表的时候被定义，而列可以随时被创建.RW键类似于关系型数据中的主键，保证行数据的完整性，Row按照RowKey排序后进行存储.四者之间的关系见下图
	![](images/操作手册（重制版）-102.png)

### 2 HBase 的HA搭建与操作
#### 2.1 HBase的HA搭建

**官网：**[Apache HBase – Apache HBase™ Home](https://hbase.apache.org/)
	![](images/操作手册（重制版）-103.png)
	![](images/操作手册（重制版）-104.png)
	![](images/操作手册（重制版）-105.png)

- 在**主节点node1**，执行一下命令,下载HBase安装文件到此目录，以下地址只做格式说明
```shell
wget https://dlcdn.apache.org/hbase/2.5.6/hbase-2.5.6-bin.tar.gz --no-check-certificate
```
- 执行以下命令，解压下载的HBase安装文件进行
```shell
tar -zxf hbase-2.5.6-bin.tar.gz -C /usr/app
```
- 打开主节点node1的环境变量文件，向文件中追加以下内容。
```shell
export HBASE_HOME=/usr/app/hbase-2.5.6
export PATH=$PATH:$HBASE_HOME/bin
```
- 访问HBase的“conf”目录，打开“conf”目录下的hbase-env.sh文件，添加以下内容。
```sehll
vi hbase-env.sh
```

```shell
export HBASE_HOME=/usr/app/hbase-2.5.6
export JAVA_HOME=/usr/java/jdk
export HADOOP_HOME=/usr/app/hadoop-3.3.6
export HBASE_PID_DIR=$HBASE_HOME/pids
export HBASE_LOG_DIR=$HBASE_HOME/logs
# 设置不适用HBase内置的ZooKeeper，而使用外部的ZooKeeper
export HBASE_MANAGES_ZK=false
# 设置时区
export TZ="Asia/Shanghai"
```
- 打开同目录下的hbase-site.xml文件，讲“value”值修改为“true”，开启分布式。
```xml
<name>hbase.cluster.distrubuted</name>
<value>true</value>
```
- 将修改hbase.tmp.dir的“value"值，指定缓存文件存储的路径。
```xml
<name>hbase.tmp.dir</name>
<value>/usr/app/hbase-2.5.6/tmp</value>
```
- 同时在此文件的</configuration\>标签前一行插入以下内容。
```xml
<!-- 指定ZooKeeper集群存放的数据的目录 -->
<property>
	<name>hbase.zookeeper.property.dataDir</name>
	<value>/opt/zookeeperdata</value>
</property>

<!-- 指定ZooKeeper集群存放数据的目录 -->
<property>
	<name>hbase.zookeeper.quorum</name>
	<value>zhanghoulin31-node1,zhanghoulin31-node2,zhanghoulin31-node3</value>
</property>

<!-- 指定HBase需要连接的ZooKeeper集群的主机名称-->
<property>
	<name>hbase:rootdir</name>
	<value>hdfs://zhanghoulin31-node1:9820/hbase</value>
</property>

<!-- 指定wal的配置，是文件系统 -->
<property>
	<name>hbase.wal.provider</name>
	<value>filesystem</value>
</property>
```
- 打开同目录下的regionservers文件
```shell
vi regionservers
```
- 删除文件中原来的内容，将集群中的三个节点主机名按以下格式写入文件中。
```shell
zhanghoulin31-node1
zhanghoulin31-node2
zhanghoulin31-node3
```
- 在当前目录下新建文件”backup-masters“，用于设置HBase的备用节点，当主机名node1崩溃时，HBase自动启动备用节点。在打开的backup-masters文件中，将从节点2的主机名写入文件中
```shell
zhanghoulin31-node2
```
- 依次执行以下命令，将主节点node1中配置好的HBase，复制到其他从节点的相同目录下。
```shell
scp -r /usr/app/hbase-2.5.6/ root@zhanghoulin31-node2:/usr/app
scp -r /usr/app/hbase-2.5.6/ root@zhanghoulin31-node3:/usr/app
```
- 在节点node2、node3上，执行以下命令，打开系统环境变量文件。
```shell
vi /etc/profile
```
- 向文件中追加写入以下内容
```shell
export HBASE_HOME=/usr/app/hbase-2.5.6
export PATH=$PATH:$HBASE_HOME/bin
```
- 保存并退出环境变量文件后，使用以下命令将新配置的环境变量内容生效
```shell
source /etc/profile
```
- 在所有节点上， 依次执行以下命令，在HBase的目录下新建”tmp“、“logs"两个子目录（用于缓存和日志）
```shell
cd /usr/app/hbase-2.5.6
mkdir tmp
mkdir logs
```

#### 2.2 启动HBase集群

- 在所有节点上，执行 以下命令，启动Zookeeper集群
```shell
zkServer.sh start
```
- 在主节点node1上，执行以下命令，启动Hadoop集群
```shell
start-dfs.sh
```
- 然后，在集群中任意一节点上，执行以下命令，查看Hadoop安全模式，
```shell
hdfs dfsadmin -safemode get
```
	如果返回的结果是以下内容，说明节点node1、node2开启了Hadoop安全模式

- 接下来，在主节点node1上执行以下命令，关闭Hadoop的安全模式
```shell
hdfs dfsadmin -safemode leave
```
	执行上述命令后，返回的信息如果是以下内容，则说明Hadoop安全模式已被关闭
- 然后在主节点node1上，依次执行以下命令，启动YARN、HBase
```shell
start-yarn.sh
start-hbase.sh
```
- 启动hbase后，会在反馈信息中显示如下信息i，表示在hadooop和habse安装目录下的log4j.jar文件发生冲突，可以忽略不记。
	![](images/操作手册（重制版）-106.png)
- 启动后，可以在所有节点上，执行”jps"，查看HBase的运行情况（在HBase HA 模式下，有两个HMaster，三个HRegionServer进程）
	![](images/操作手册（重制版）-107.png)
	![](images/操作手册（重制版）-108.png)
	![](images/操作手册（重制版）-109.png)

- HBase的Master和RegionServer的http端口分别是16010和16030，因此在浏览器地址栏分别输入“http://node1:16010”和"http:\//node2:16010",打开HBase的Master的WEB页面，分别输入“http:/\/node1.16030"、”http:/\/node2.16030“和”http:/\/node3.16030“可以打开三个节点各自的Regionserver的WEB页面。
	![](images/操作手册（重制版）-110.png)
	![](images/操作手册（重制版）-111.png)
	![](images/操作手册（重制版）-112.png)


## 任务4.2 HBase的常用操作

对HBase的操作，分为**表操作**和**数据操作**，均在HBase Shell启动状态下完成。在集群中的任意一节点上，执行以下命令，启动HBase Shell。
```shell
hbase shell
```
1. 表操作
	- **建表**的语法格式：
		 **create '表名称', '列簇名称1', '列簇名称2', '列簇名称N'**
		 例如，创建一个带有一个列簇，表名为“test"的数据表
		 **create 'test','cf1'**

	 - 可以通过”list“**查看**HBase下的所有数据表
		 **list**

	 - **删除**表之前，需要先将表进行”disable“，然后通过” drop“，才能删除表。
		 **disable 'test'**
		 **drop 'test'**

2. 数据操作
	（1）**添加数据**
		 语法格式：
			 put '表名称'， 'ROW KEY 值'，'列簇名称：列名'，'列值'
		 例如，向’test‘表中添加多条记录。
			 **put 'test', '1', 'cf1:id', 1**
			 **put 'test', '1', 'cf1:name', 'jack'**
			 **put 'test', '2', 'cf1:id', 2**

	（2）查询数据表内容
		 语法格式：
			 scan '表名称'
		 例如，查询’test‘表中的内容。
			 **scan 'test'**

	（3）查询表中某行数据
		 语法格式：
			 get '表名称', 'ROW KEY 值'
		 例如， 查询‘test’表中，行键值为1的记录。
			 get 'test', '1'

	（4）修改表中数据
		 语法格式：
			 put '表名称','ROW KEY值', '列簇名称：列名', '新列名'
		 例如，将’test'表中的第一条记录中”name"的值，修改为“tom"
			 put 'test', '1', 'cf1:name', 'tom'

	（5）删除某个数据
		 语法格式：
			 delete '表名称', 'ROW KEY 值', '列簇名称：列名'
		 例如，删除‘test’表中，航舰为1的记录中，‘cf1’列簇下的‘name'列的值。
			 delete ’test', '1', 'cf1:name'


## 任务4.3 HBase常见异常处理与维护

如果，在启动HBase集群后，发现HMaster进程自动退出，没有出现在”jps"的反馈结果中，可以在主节点node1的HBase目录下，执行以下命令，查看“logs”目录下的“HMaster”日志文件。
```shell
tail -100 logs/hbase-root-master-node1.log
```
- 如果在日志中显示有以下内容，说明是因为节点node1处于standby状态而造成的问题。
```shell
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RemoteExcption): Operation category READ is not supported in state standby. Visit https://s.apache.org/sbnn-eror
```
	- 解决这个问题的步骤如下。
	1. 在节点node2上，执行以下命令，关闭节点node2中的namenode进程。执行以下命令后，最好用jps命令，确定一次namenode进程是否已经退出
	hdfs --daemon stop namenode
	2. 接下来，执行以下命令，查看节点node1的namenode状态，从返回结果信息中，确认节点node1的namenode状态是否已经转变成了active。如果没有，多试几次，因为有可能因为退出进程正在运行，而系统反应迟缓
	hdfs haadmin -getServiceState nn1
	active
	3. 在节点node1的namenode状态转变成active后，在节点node2上，执行以下命令，恢复namenode进程
	hdfs --daemon start namenode
	这时，如果执行以下命令，会发现节点node2上的namenode状态转变成了standby。
	hdfs haadmin -getServiceState nn2
	standby
	4. 最后在主节点node1上，重新执行以下命令，启动HBase集群即可
	start-hbase.sh




# 项目五 数据仓库工具Hive
## 5.1 什么是Hive

Hive是一个基于 Hadoop平台，底层采用MapReduce 实现对数据的HDFS存储、分析和处理的数据仓库管理工具。所以Hive不是数据库，它只是一款封装了MapReduce计算框架的工具，实现对HDFS数据的分析和管理。Hive的使用价值在于，它提供了类似SQL的查询功能，使用者可以不用直接接触到MapReduce程序，就能实现对数据的统计分析。
**1.Hive 的优势**
	(1)通过HQL语言(hive query language),可以轻松地访问数据，支持数据仓库任务，如提取、转换、加载(ETL)、报告和数据分析功能。
	(2)可以直接访问存储在HDFS中的数据，或其他基于Hadoop平台的数据存储系统，比如HBase。
	(3)适合熟悉SQL语句，但不会MapReduce编程的数据分析人员使用，适用于离线的批量数据计算。
**2.Hive 的尖势**
	(1)虽然Hive可以取代一部分编写MapReduce程序的工作，但Hive 只能做一些简单的统计查询分析，想要做复杂的数据挖掘，仍然需要编写 MapReduce 程序;
	(2)Hive 使用的HQL语言，是一款类似关系型数据库的SQL语句，所以Hive适合处理存在有关系的数据。


## 5.2 安装MySQL

### 5.2.1 安装MySQL的开源MariaDB版

执行以下命令安装mariadb命令
```shell
yum install -y mariadb-server
```

### 5.2.2 进行字符集设置，以兼容中文

在主节点node1上，执行以下命令，打开”my.cnf“文件
```shell
vi /etc/my.cnf
```
在”my.cnf“文件的”\[mysqld]“标签下添加以下内容。
```shell
init_connect='SET collation_connection = utf8_unicode_ci'
init_connect='SET NAMES utf8'
character-set-server=utf8
collation-server=utf8_unicode_ci
skip-character-set-client-handshake
```
在”\[mysqld_safe]“标签下添加以下内容。
```shell
skip-grant-tables
```
在主节点node1上，执行以下命令，打开”client.cnf“文件。
```shell
vi /etc/my.cnf.d/client.cnf
```
在”client.cnf“文件的”\[client]“标签下添加以下内容
```shell
default-character-set=utf8
```
在主节点node1上，执行以下命令，打开”mysql-clients.cnf“
```shell
vi /etc/my.cnf.d/mysql-clients.cnf
```
在”mysql-clients.cnf“文件的”\[mysql]“标签下添加以下命令。
```shell
default-character-set=utf8
```

### 5.2.3 启动服务

在主节点node1上，执行以下命令，启动MariaDB-MySQL服务。
```shell
systemctl start mariadb
```
执行以下命令，将MariaDB-MySQL服务设置成开机启动。
```shell
systemctl enable mariadb
```

### 5.2.4 访问测试

在主节点node1上，执行以下命令，登录”mariadb-mysql“。
```shell
mysql -u root -p
```
如果在反馈信息中，显示如下输出内容，表明登录”mariadb-mysql“成功。
	![](images/操作手册（重制版）-113.png)
在光标出输入”quit“，可以退出”mariadb-mysql“。
```mysql
quit
```

### 5.2.5 远程访问MySQL

1. 远程访问前的查看
	在windows系统，按”win+r“组合键，打开”运行“窗口，在窗口中输入”cmd“，然后按”确定“按钮.
	![](images/操作手册（重制版）-114.png)
	在弹出的窗口中输入”ipconfig“，并按回车键
	![](images/操作手册（重制版）-115.png)
	在反馈结果中找到”VMware Network Adapter VMnet8“位置，记住VMnet8的IPv4地址。
	![](images/操作手册（重制版）-116.png)

2. 远程访问前的设置
	回到主节点node1，执行以下命令，登录”mariadb-mysql“
```shell
mysql -u root -p
```
执行以下SQL语句，访问”mariadb-mysql“中的”mysql“数据库。
```mysql
use mysql;
```
执行以下SQL语句，查看”mysql“数据库中的”user“表
```mysql
select host,user from user;
```
查询结果如图
	![](images/操作手册（重制版）-117.png)
执行以下SQL语句，向表中插入一条新纪录（代码中的”\`\*.\*.\*.2\`“是前面查到的VMnet8的IPv4地址，这里只作为语法格式说明，不代表实际地址）
```
insert into user(`host`,`user`) value ('192.168.40.1', 'root');
insert into user(`host`,`user`) value ('192.168.137.1', 'root');
```
再次执行一次”user“表的查询操作。
```mysql
select host,user from user;
```
查询结果如图所示，可以看到在查询结果中已经增加了一条新纪录。
	![](images/操作手册（重制版）-118.png)
执行以下SQL语句，修改”root"用户的密码，密码为“root”.
```mysql
update user set password=password('root') where user='root';
```
执行以下命令，刷新“mysql“权限表
```mysql
flush privileges;
```
退出”mysql“。
```mysql
quit
```
执行以下命令，打开主节点node1的”my.cnf“配置文件
```shell
vi /etc/my.cnf
```
将”my.cnf“配置文件的”\[mysqld_safe]“标签下的”skip-grant-tables“注释掉。
```conf
[mysqld_safe]
# skip-grant-tables
```
执行以下命令，重启”mariadb-mysql“服务
```shell
systemctl restart mariadb
```
3. 远程访问操作
打开navicat工具，依次选择“连接”=> “MariaDB”选项
	![](images/操作手册（重制版）-119.png)
在弹出的“新建连接设置”窗口，填写相关连接信息，其中“node1”是主节点node1的主机名，密码是“root"/
	![](images/操作手册（重制版）-120.png)
填写后，单击左下角”测试连接“按钮，测试填写的连接是否有误。
	![](images/操作手册（重制版）-121.png)
单击弹出的”连接成功”窗口中的“确定”按钮，返回原来的“新建连接设置”窗口，单击窗口上的"确定“按钮，返回navicat工具的主界面窗口，在左侧窗口中出现刚刚新建的”mariadb“连接，双击此连接名和连接名下的”test"数据库，显示情况如图
	![](images/操作手册（重制版）-122.png)
至此，远程连接“mysql"成功，现在可以在Navicat工具中，以窗口的方式来操作”mysql“了。

## 5.3 安装与配置Hive

网址：[Downloads (apache.org)](https://hive.apache.org/general/downloads/)
	![](images/操作手册（重制版）-123.png)
	![](images/操作手册（重制版）-124.png)
	![](images/操作手册（重制版）-125.png)
	![](images/操作手册（重制版）-126.png)

进入‘/usr/software'目录
```shell
cd /usr/software
```
下载Hive
```shell
wget https://dlcdn.apache.org/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz
```
解压
```shell
tar -zxf apache-hive-3.1.2-bin.tar.gz -C /usr/app/
```
在”/usr/app“目录下，重命名
```shell
mv apache-hive-3.1.2-bin hive-3.1.2-bin
```

### 5.3.2 设置环境变量

在node1上，打开环境配置文件
```shell
vi /etc/profile
```
在文件内，追加一下内容
```shell
export HIVE_HOME=/usr/app/hive-3.1.2-bin
export PATH=$PATH:$HIVE_HOME/bin
```
使设置生效
```shell
source /etc/profile
```

### 5.3.3 配置Hive

进入’conf‘目录
```shell
cd /usr/app/hive-3.1.2-bin/conf
```
复制并重命名这个两个文件
```shell
cp hive-default.xml.template hive-site.xml
cp hive-env.sh.template hive-env.sh
```
打开’hive-site.xml‘文件
```shell
vi hive-env.sh
```
开头插入
```shell
HADOOP_HOME=/usr/app/hadoop-3.3.6
```
打开’hive-site.xml‘
```shell
vi hive-site.xml
```
键盘按下”/“键，进入vi编辑器的底行模式，在”/“字符后输入”org.apache.derby.jdbc.EmbeddedDriver"后，按回车键，找到“org.apache.derby.jdbc.EmbeddedDriver”所在的位置。
	![](images/操作手册（重制版）-127.png)
键盘按下“ESC”键，退出底行模式，然后按下“i”键，进入vi编辑模式，将“org.apache.derby.jdbc.EmbeddedDriver"修改为”com.mysql.cj.jdbc.Driver“,修改后的效果如图
	![](images/操作手册（重制版）-128.png)
继续采用查找方式，在文件中找到以下连个内容的位置。（584-1127）
```xml
<value>jdbc:derby:;databaseName=metastore_db;create=true</value>
------
<value>APP</value>
```
将以上查找的内容，分别修改为以下两个内容，代码中的”node1“为主节点node1的主机名。
```xml
<value>jdbc:mysql://zhanghoulin31-node1:3306/hive?createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=UTF-8&mp;useSSL=false</value>
------
<value>root</value>
```
在底行模式下，输入以下内容，采用精确查找方式，找到文件中“mine”出现的地方
```shell
/\>mine\<
```
将“mine”修改为“root”。
	修改前
	![](images/操作手册（重制版）-129.png)
	修改后
	![](images/操作手册（重制版）-130.png)

键盘按下“ESC”键，退出编辑模式后，按下“:”（英文的冒号），进入底行模式，输入以下内容，采用vi编辑器的替换功能，将文件中的“{system:java.io.tmpdir}”替换成“/root/hivetemp”
```shell
%s#${system:java.io.tmpdir}#/root/hivetemp#g
```
继续采用这种方式，在底行模式下，输入以下内容，将“{system:user.name}”替换成“root”
```shell
%s#${system:user.name}#root#g
```
执行后，底行会返回如下信息，表示替换成功
```
3 substitutions on 3 lines
```
键盘继续按下“:”（英文的冒号），进入底行模式，输入以下内容，显示行号
```shell
set number
```
然后在键盘上敲击“3291G”，将光标定位到文中的以下代码处。
```xml
<description>
      Ensures commands with OVERWRITE (such as INSERT OVERWRITE) acquire Exclusive locks for&#8;transactional tables.  This ensures that inserts (w/o overwrite) running concurrently
      are not hidden by the INSERT OVERWRITE.
</description>
```
将以上代码用注释符“\<!-- -->”注释掉，注释后的效果如下。
```xml
    <!-- <description>
      Ensures commands with OVERWRITE (such as INSERT OVERWRITE) acquire Exclusive locks for&#8;transactional tables.  This ensures that inserts (w/o overwrite) running concurrently
      are not hidden by the INSERT OVERWRITE.
    </description> -->
```
保存并退出“hive-site.xml”文件后，然后执行以下命令，在“/root”目录下，新建目录“hivetemp”。
```shell
mkdir /root/hivetemp
```
执行以下命令，进入“/usr/software”目录。
```shell
cd /usr/software
```
执行以下命令，将MySQL数据库的java驱动文件下载到该目录。
```shell
wget https://cdn.mysql.com/archives/mysql-connector-java-8.0/mysql-connector-java-8.0.18.tar.gz
```
执行以下命令，解压下载的MySQL数据库的java驱动文件
```shell
tar -zxf mysql-connector-java-8.0.18.tar.gz
```
执行以下命令，进入解压后的”mysql-connector-java-8.0.18"目录
```shell
cd mysql-connector-java-8.0.18
```
执行以下命令，将当前目录下的”mysql-connector-java-8.0.18.jar“文件复制到Hive的”lib“目录中，注意在”jar“字符后，要留有一个英文空格，与后面的目录内容间隔开。
```shell
cp mysql-connector-java-8.0.18.jar /usr/app/hive-3.1.2-bin/lib/
```
执行以下命令，进行Hive的元数据（Metastore）初始化。
```shell
schematool -initSchema -dbType mysql
```
![](images/操作手册（重制版）-131.png)
再次访问“mariadb”，验证Hive的初始化是否成功。
```shell
mysql -uroot -proot
```
查看“mariadb”下的所有数据库。
```mysql
show databases;
```
显示Hive数据库，表示Hive的元数据初始化成功。
![](images/操作手册（重制版）-132.png)
查看后，可退出”mariadb“。
```mysql
quit
```


### 5.3.4 启动Hive

1. Beeline的设置
   进入主节点node1的”/usr/app/hadoop-3.3.6/etc/hadoop“目录
```shell
cd /usr/app/hadoop-3.3.6/etc/hadoop/
```
打开目录下的“core-site.xml"文件
在”\</configuration>“标签前另起一行，插入下列内容，进行权限的设置/
```xml
<property>
	<name>hadoop.proxyuser.root.hosts</name>
	<value>*</value>
</property>

<property>
	<name>hadoop.proxyuser.root.groups</name>
	<value>*</value>
</property>
```
打开同目录下的”hdfs-site.xml“文件。
```shell
vi hdfs-site.xml
```
在”\</configuration>“标签前另起一行，插入下列内容。
```xml
<property>
	<name>dfs.permissions</name>
	<value>false</value>
</property>
```
依次执行以下命令，将以上两个被修改后的”core-site.xml"文件和“hdfs-site.xml”文件，复制到其他两个从节点的相同目录下。
```shell
scp core-site.xml hdfs-site.xml root@zhanghoulin31-node2:$PWD
scp core-site.xml hdfs-site.xml root@zhanghoulin31-node3:$PWD
```
依次执行以下命令，将主节点node1上的hive复制到其他两个从节点的相同目录下。
```shell
scp -r /usr/app/hive-3.1.2-bin/ root@zhanghoulin31-node2:/usr/app/
scp -r /usr/app/hive-3.1.2-bin/ root@zhanghoulin31-node3:/usr/app/
```
在从节点node2、node3上，打开环境变量配置文件。
```shell
vi /etc/profile
```
向文件内，追加以下内容。
```shell
export HIVE_HOME=/usr/app/hive-3.1.2-bin
export PATH=$PATH:$HIVE_HOME/bin
```
在从节点node2、node3上，执行命令，使设置生效。
```shell
source /etc/profile
```

2. 启动Hive服务端
在启动HIve前，需要先启动hadoop的HA模式，在所有节点上，执行以下命令，启动zookeeper集群。
```shell
zkServer.sh start
```
在主节点node1上，依次执行以下命令，启动hadoop集群。
```shell
start-dfs.sh
start-yarn.sh
```
在所有节点上，依次执行以下命令，启动历史查看服务。
```shell
mapred --daemon start historyserver
yarn --daemon start timelineserver
```
关闭主节点node1上hadoop的安全模式。
```shell
hdfs dfsadmin -safemode leave
```
在主节点node1启动hive2服务端
```shell
hiveserver2
```
反馈的信息中显示有三个“Hive Session”在运行，说明Hive2服务端已处于等待客户端接入状态。
	![](images/操作手册（重制版）-133.png)
	![](images/操作手册（重制版）-138.png)
此时，如果在浏览器地址栏中访问“http://zhanghoulin31-node1:10002/"地址，能打开如图，说明成功。
	![](images/操作手册（重制版）-134.png)


3. 启动Hive的客户端
在hadoop集群的其他任意一节点上，以从节点node3为例，执行以下命令，用Beeline启动HIve的客户端
```shell
beeline
```
启动后，在Beeline命令模式下输入连接服务端信息，Hive2的服务端默认端口号为”10000“。
```shell
!connect jdbc:hive2://zhanghoulin31-node1:10000
```
连接服务端后，当命令行提示输入用户名、密码时，依次输入管理员的用户名”root“、密码为空，不输入，直接按回车。
```shell
Enter username for jdbc:hive2://zhanghoulin31-node1:10000: root
Enter password for jdbc:hive2://zhanghoulin31-node1:10000:
```
成功登录后会显示如图
![](images/操作手册（重制版）-135.png)
在命令行输入”show databases;“会显示如图
![](images/操作手册（重制版）-136.png)
刷新如图的web页面，可以看到页面中显示了连接到服务器的客户端信息，以及操作Hive SQL 命令的历史记录。
![](images/操作手册（重制版）-137.png)
但改界面目前功能单一，只用于展示Hive服务的相关信息，页面上没有提供对Hive的其他操作功能，在Hive客户端，执行以下命令，可以退出Beeline。
```shell
!quit
```
按”ctrl+c“组合键，也可以退出Hive的客户端、服务器。执行”hdfs dfsadmin -safemode enter“命令可恢复hadoop的安全模式。


## 5.4 Hive数据操作


**HIVE中包含的数据模型**

| 序号 | 数据模型名称 | 说明 |
| :---: | :---: | :---: |
| 1 | Database（数据库） | 在HDFS中表现为在”hive-site.xml“配置文件中属性”hive.metastore.warehouse.dir“所设置目录下一个文件夹，用于避免表、视图、分区、列等的命令冲突 |
| 2 | Table（数据表） | 在HDFS中表现为所属database目录下的一个子文件夹 }
| 3 | External Table（外部表） | 与table类型，不过其数据存放位置可以执行任意HDFS目录路径 |
| 4 | Partition（分区） | 在HDFS中表现为table目录下的子目录 |
| 5 | Bucket（桶） | 在HDFS中表现为同一个表目录或者分区目录下根据某个字段的值进行hash散列之后的多个文件 |
| 6 | View（视图） | 与传统数据库类似，只读，基于数据表创建，Hive中的视图仅仅相当于一个SQL的快捷方式或别名 |


**Hive支持的数据类型**

| 序号 | 类别 | 数据类型 | 长度 | 说明 |
| :---: | :---: | :---: | :---: | :---: |
| 1 | 数字型 | TINYINT | 1字节 | 有符号整型，范围从-128到127 |
| 2 |  | SMALLINT | 2字节 | 有符号整型，范围从-32768到32767 |
| 3 |  | INT | 4字节 | 有符号整型，范围从-2147483648到2147483647 |
| 4 |  | BIGINT | 8字节 | 有符号整型，范围从-9223272036854775808到9223372036854775807 |
| 5 |  | FLOAT | 4字节 | 有符号单精度浮点数 |
| 6 |  | DOUBLE | 8字节 | 有符号双精度浮点数 |
| 7 |  | DECIMAL | 用户自定义精度和数据规模大小 | 可带小数的精度数字，从Hive 3.0.0开始，引入NUMERIC，等同于DECIMAL |
| 8 | 日期型 | TIMESTAMP | -- | 时间戳，语法格式：yyyy-mm-dd hh:mm:ss\[.f...] |
| 9 |  | DATE | -- | 日期，语法格式：YYYYMMDD |
| 10 |  | INTERVAL | -- | 时间间隔长度，单位有：SECOND/MINUTE/DAY/MONTH/YEAR,比如INTERVAL'1' DAY, 表示间隔一天时间长度 }
| 11 | 字符型 | STRING | -- | 字符串 |
| 12 |  | VARCHAR | 字符数范围1~65535 | 长度不定字符串 |
| 13 |  | CHAR | 最大字符数：255 | 长度固定字符串 |
| 14 | 复合型 | ARRAY | -- | 包含同类型元素的数组，索引从0开始ARRAY\<data_type> |
| 15 |  | MAP | -- | 字典MAP\<primitive_type，data_type> |
| 16 |  | STRUCT | -- | 结构体STRUCT\<col_name:data_type\[COMMENT col_comment],...> |
| 17 | 其他 | BOOLEAN | -- | 布尔类型TRUE/FALSE |
| 18 |  | BINARY | -- | 字节序列 |



### 5.4.1 数据库操作

执行以下HQL语句，创建数据库”test“。
```sql
create database test;
```
查询Hive中的所有数据库
```sql
show databases;
```
如果成功，会反馈以下信息。

指定使用‘test’数据库（如果不指定，默认使用default数据库）。
```sql
use test;
```
如果要删除‘test’数据库，可以执行以下HQL语句。
```sql
drop database if exists test cascade;
```

#### 5.4.2 数据表操作

1. 表的创建
   Hive SQL提供了以下三种操作方式：分步操作方式、单行操作方式、脚本文件操作方式（这三种操作方式，在实际操作中任选一种即可），这里以脚本文件操作方式为例进行讲解。

为了操作的需要，在创建表之前，准备一份要添加到数据表中的数据。首先在主节点node1的”/usr/test“目录下，新建一文本文件”data.txt“，在输入时，每一列的数据之间必须间隔一个”tab“键的宽度（制表符的宽度），在”data.txt“文件中写入以下内容。
```
1       rod     18      study-game-driver       std_addr:beijing-work_addr:shanghai
2       tom     21      study-game-driver       std_addr:beijing-work_addr:beijing
3       jerry   33      study-game-driver       std_addr:beijing-work_addr:shenzhen
```
对于复杂的HQL的内容，Hive SQL提供了以下三种操作方式（这三种操作方式，在实际操作中任选一种即可）。接下来的“person”表的创建，将按符合“data.txt"中的数据结构创建。
- 分布操作
```sql
-- 指定目标数据库
use test;
-- 创建person表，设计表中的字段名，以及数据类型
create table person(id int, name string, age int, fav array<string>, addr map<string, string>) 
-- 指定数据的序列化格式规则，字段之间以制表符间隔
row format delimited fields terminated by '\t' 
-- 集合类型的数据以'-'间隔
collection items terminated by '-' 
-- 字典类型的数据以':'间隔
map keys terminated by ':' 
-- 数据在HDFS中，以文本文件的方式保存
stored as textfile;
```

- 单行操作
```
create table person(id int, name string, age int, fav array<string>, addr map<string, string>) row format delimited fields terminated by '\t' collection items terminated by '-' map keys terminated by ':' stored as textfile;
```

首先在主节点node3的”/usr/test/hive-test“目录下，新建一文本文件”person-table.sql“，在”person-table.sql“文件中写入以下内容。
- 脚本模式
```sql
use test;

create table student(id int, name string, age int, fav array<string>, addr map<string, string>) 
row format delimited fields terminated by '\t' 
collection items terminated by '-' 
map keys terminated by ':' 
stored as textfile;
```

在beeline命令模式下，执行以下命令，运行“person-table.sql”脚本文件。命令的语法格式如下。
```
!run  带绝对路径的脚本文件名
```
命令如下
```
!run /usr/test/hive-test/person-table.sql
```
删除表
```sql
drop table student;
```


2. 数据加载到表
加载数据到Hive表中有两种方式。
(1)从本地文件中加载数据到表中
执行以下HQL语句，将“data.txt”文件中的原始数据加载到新建的“person”表中，因为在主节点node1上启动的Hive服务端，因此“data.txt”文件应放在节点node1中。
```sql
-- load data local inpath 关键字 
load data local inpath '/usr/test/data.txt' into table person;
```
(2)从HDFS中加载数据到表中
在加载数据前，在HDFS上应已有准备好的数据。执行以下HQL语句，将HDFS中的数据加载到Hive的“person”数据表中，如果在into关键字前加入overwrite关键字则是覆盖写，表中的原有数据会被删除。
```
load data inpath '/test_data/data.txt' into table person;
```
执行后，HDFS中的“data.txt”文件会被移动到“person”表所在HDFS中的目录下。



3. 表数据查询
执行以下HQL命令，查询表数据
```sql
select * from person;
```

表的重命名，例如将”person“表重命名为”person_new“
```sql
alter table person rename to person_newe;
```


#### 5.4.3 表的分区与分桶

1. 分区
执行一下HQL语句，创建一个分区数据表”person_partition“,语句中以”mytime“字段作为分区列，注意”partition by （mytime string）“分区代码不能放置在”row format delimited“代码后，否则无法识别，分区要在建表时建立。
```sql
CREATE TABLE person_partition (
  id INT,
  name STRING,
  age INT,
  fav ARRAY<STRING>,
  addr MAP<STRING, STRING>
)
PARTITIONED BY (mytime STRING) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\t' 
COLLECTION ITEMS TERMINATED BY '-' 
MAP KEYS TERMINATED BY ':' 
STORED AS TEXTFILE;
```
在打开一个主节点node1的连接终端窗口，执行一下命令，将主节点node1的”/usr/test“目录下的文件”data.txt"，上传到HDFS，重命名为“person.txt”
```shell
hdfs dfs -put /usr/test/data.txt /test_data/person.txt
```
回到Beeline端，执行以下HQL语句，将数据添加到时间为“2018-01-01”的分区中
```sql
load data inpath '/test_data/person.txt' into table person_partition partition (mytime='2018-01-01');
```
执行以下命令，进行分区查询。
```sql
select id, name, addr['work_addr'] as work_addr from person_partition where mytime='2018-01-01';
```
![](images/操作手册（重制版）-139.png)

2. 分桶
分桶时相对分区进行更小区域的划分范围，适用于分区中数据过大时用，分桶 同样要在建表时建立，执行以下HQL语句，以“id"字段为分桶依据，创建数据表”person_buckets“，将每个分区分成3个桶。
```sql
create table person_buckets(
id int,
name string,
age int,
fav array<string>,
addr map<string,string>
)
partitioned by (mytime string)
clustered by (id)
sorted by (name)
into 3 buckets
row format delimited
fields terminated by '\t'
collection items terminated by '-'
map keys terminated by ':'
stored as textfile;
```
执行以下HQL语句，从其他相同结构的普通表中向分桶表导入数据
```sql
insert into table person_buckets partition (mytime='2017-01-01') select * from person;
```
反馈的信息中如果出现以下内容，则表明导入数据成功
![](images/操作手册（重制版）-141.png)
执行以下HQL命令，按桶号方式，查询分桶表
```sql
select * from person_buckets tablesample (bucket 2 out of 3 on id);
```
查询结果如下
![](images/操作手册（重制版）-140.png)
按全局方式，查询分桶表（这种方式一般不提倡用于分桶表，之所以分桶，肯定时因为数据量非常大，才需要进行分桶）
```sql
select * from person_buckets;
```
查询结果如下
![](images/操作手册（重制版）-142.png)


#### 5.4.4 内部表与外部表

    Hive数据表分为内部表（managed table）和外部表（external table），内部表在创建时会把数据移动到Hive指定的位置，外部表则仅记录数据所在的位置。在删除的时候，内部表会将元数据和数据一起删除，而外部表仅删除元数据，真正的数据不会删除。因此，在共享数据源的情况下，可以选择创建外部表，如果仅仅只供Hive内部使用，则使用内部表。
    内部表数据由Hive自身管理，外部表数据由HDFS管理。内部表数据存储的位置是由Hive的“hive-site.xml”配置文件中的“hive.metastore.warehouse.dir”属性值决定的，默认值是“/user/hive/warehouse”。
    外部表数据的存储位置由创建外部表时，在HQL语句中的“LOCATION”指定。如果在创建外部表时，没有指定外部表数据的存储位置，Hive将在HDFS上的“/user/hive/warehouse”文件夹下以外部表的表名创建一个文件夹，并将属于这个表的数据存放于此。

1. 内部表
内部表的创建时Hive表的默认创建方式，在前面5.4.2 数据表操作一节中描述的内容，即是内部表的创建过程。
在Beeline命令模式下，执行命令，选择”test“数据库，如果已选择过，可以省略此步骤。然后执行以下HQL语句，查看”person“表的结构。
```sql
desc formatted person;
```
如图所示，从反馈的信息中，可以看到表的类型为”MANAGED_TABLE“，表明”person“表是一个内部表。
![](images/操作手册（重制版）-143.png)


2. 外部表
- 外部表的创建
  创建外部表的语法规则：
```sql
create external table 表名（表的结构信息）
```
例如，在Hive的”test“数据库下，创建一张外部表，表名为”person_external“。为了重用”data.txt”文件中的数据，除了表名，以及“external”关键字外，表的结构与前面的“person”内部表的结构完全一致，在Beeline命令模式下，执行以下HQL语句。
```sql
create external table person_external(
id int,
name string,
age int,
fav array<string>,
addr map<string,string>
)
row format delimited
fields terminated by '\t'
collection items terminated by '-'
map keys terminated by ':'
stored as textfile;
```
执行以下HQL语句，查看“person_external”表的结构。
```sql
desc formatted person_external;
```
![](images/操作手册（重制版）-144.png)
执行show tables语句后，反馈信息显示，“person_external”外部表创建成功。

```sql
show tables;
```
![](images/操作手册（重制版）-145.png)

- 外部表数据的加载
执行以下HQL语句，将主节点node1的本地文件“data.txt”中的数据导入“person_external“外部表中。
```sql
load data local inpath "/usr/test/data.txt" into table person_external;
```
执行 以下HQL语句，查询数据是否成功导入”person_external“外部表。
```sql
select * from person_external;
```
查询结果如下，表明数据成功导入。
![](images/操作手册（重制版）-146.png)

3. 关联HBase表
操作前，需要依次启动好Zookeeper、HDFS、Yarn、HBase、Hive服务。
创建一个Hive外表去关联HBase表的目的，是为了方便使用Hive的HQL语句来处理HBase中存放的数据，而不用直接写MapRseduce程序。当Hive外表与HBase数据表关联时，数据是存放在HBase数据表中，而HDFS中只会创建一个Hive表的目录，并不会有具体的数据文件。
进入”hbase shell"命令模式，执行以下命令，确认HBase中是否已存在“persons-test”表。
```shell
list
```
如果存在，依次执行以下命令，进行表的删除
```
disable 'persons-test'
drop 'persons-test'
```
然后执行以下命令，重新进行“persons-test”表的创建。
```
create 'persons-test', 'f1'
```
创建后，再次执行以下命令，确认是否创建成功
```
list
```
成功后，退出“hbase shell”命令模式，分别开启Hive服务端和客户端。在Hive客户端登录成功后，执行以下HQL语句，选择前面建好的“test”数据库。
```sql
use test;
```
接下来 ，在Hive中 创建一个外部表“persons_hbase”，去关联HBase中的“‘persons-test’”表。
语法规则如下：
```sql
create external table 表名（表结构）stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' with serdeproperties('hbase.columns.mapping'=':key,Hbase 表的列簇名: Hive表的字段名 ...' tblproperties('hbase.table.name'='HBase表名'))
```
具体代码如下
```sql
create external table persons_hbase(
id int,
name string,
age int,
fav array<string>,
addr map<string,string>
)
stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' with serdeproperties('hbase.columns.mapping'=':key, f1:name, f1:age, f1:fav, f1:addr') tblproperties('hbase.table.name'='persons-test');
```
创建后，执行以下HQL语句，从反馈结果中，核对“persons_hbase”表是否创建成功。
```sql
show tables;
```
查询结果如下
![](images/操作手册（重制版）-147.png)
接下来，执行以下HQL语句，从前面已存在的“person”表 中，给“persons_hbase”表加载数据。
```sql
insert overwrite table persons_hbase select * from  person;
```
反馈信息如下，表明执行成功。
![](images/操作手册（重制版）-148.png)
执行以下HQL语句，查询“persons_hbase”表中的数据是否存在。
```sql
select * from persons_hbase;
```
查询结果如下。
![](images/操作手册（重制版）-149.png)
依次退出Hive的客户端和服务端，重新进入“hbase shell”命令模式，执行以下命令，查看“person-test”表中是否存在以下数据。

```sql
scan 'persons-test'
```
查询结果如下，表明“persons-test”表中已加载到数据。
![](images/操作手册（重制版）-150.png)





